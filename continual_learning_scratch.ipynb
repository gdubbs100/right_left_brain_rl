{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch development for continual learning loop\n",
    "\n",
    "What I want to do:\n",
    "- There is a continual learning env that lets an agent learn on a stream of tasks\n",
    "- At specified intervals, we can evaluate the continual learner on _every_ task in the training set\n",
    "\n",
    "Things to consider:\n",
    "- can I use the ML10 environments in this setting? goals are obscured, what does that mean?\n",
    "- how many steps per env do I want? I think CW uses 1 million? check this, also, with PPO is this enough? (CW uses SAC)\n",
    "- randomisation of task goals - CW seems to have randomisation settings for the benchmark. MT and ML have randomised environments. How should we handle these tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other thoughts\n",
    "Next step is to integrate the continual environment and evaluation loop into a training loop. We want to use the PPO algorithm provided (or something similar), but provide it with arbitrary policy / value networks (i.e. our own).\n",
    "\n",
    "Testing learning in this way might require setting up some command line 'args' stuff - might be easier to match it with PPO\n",
    "But is this the simplest testing method?\n",
    "\n",
    "Should look at policy storage - how can I use that?\n",
    "\n",
    "Also - continue to look at how to get continual environments to work. Continual world has randomization handlers, success counters etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('./algorithms/')\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.custom_ppo import CustomPPO\n",
    "from models.combined_actor_critic import ActorCritic\n",
    "RUN_FOLDER = './rl2_baseline/rl2_double_baseline'\n",
    "policy_net_agent = torch.load(RUN_FOLDER + '/models/policy.pt', map_location=device)\n",
    "encoder_net_agent = torch.load(RUN_FOLDER + '/models/encoder.pt', map_location=device)\n",
    "actor_critic = ActorCritic(policy_net_agent, encoder_net_agent)\n",
    "\n",
    "# agent = CustomPPO(\n",
    "#                  actor_critic=actor_critic,\n",
    "#                  value_loss_coef=1,\n",
    "#                  entropy_coef=1,\n",
    "#                  policy_optimiser='Adam',\n",
    "#                  lr=0.1,\n",
    "#                  clip_param=0.2,\n",
    "#                  ppo_epoch=5,\n",
    "#                  num_mini_batch=5,\n",
    "#                  max_grad_norm = 0.5,\n",
    "#                  eps=None,\n",
    "#                  use_huber_loss=True,\n",
    "#                  use_clipped_value_loss=True,\n",
    "#                  context_window = None\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create envs\n",
    "from environments.custom_env_utils import prepare_parallel_envs, prepare_base_envs\n",
    "from environments.custom_metaworld_benchmark import ML3\n",
    "num_processes=2\n",
    "raw_envs = prepare_base_envs(\n",
    "    ['push-v2'], benchmark=ML3(), task_set='train', randomization='deterministic')\n",
    "    # task_names, benchmark = MT50, task_set = 'train', randomization=\"random_init_fixed20\"\n",
    "envs = prepare_parallel_envs(\n",
    "    envs = raw_envs,\n",
    "    steps_per_env=500,\n",
    "    num_processes=num_processes,\n",
    "    gamma=0.99,\n",
    "    normalise_rew=True,\n",
    "    device=device,\n",
    "    seed = 73\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy_agent1:\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic\n",
    "\n",
    "    def act(self, state, latent, belief, task, deterministic = False):\n",
    "        return self.actor_critic.act(state, latent, belief, task, deterministic)\n",
    "    \n",
    "    def get_value(self, state, latent, belief, task):\n",
    "        return self.actor_critic.get_value(state, latent, belief, task)\n",
    "    \n",
    "    def get_latent(self, action, state, reward, hidden_state, return_prior = False):\n",
    "        _, latent_mean, latent_logvar, hidden_state = self.actor_critic.encoder(action, state, reward, hidden_state, return_prior = return_prior)\n",
    "        latent = torch.cat((latent_mean.clone(), latent_logvar.clone()), dim = -1).squeeze()\n",
    "        ## assume always add non-linearity to latent\n",
    "        # return F.relu(latent[None,:]), hidden_state\n",
    "        return F.relu(latent), hidden_state\n",
    "    \n",
    "    def get_prior(self, num_processes):\n",
    "        _, latent_mean, latent_logvar, hidden_state = self.actor_critic.encoder.prior(num_processes)\n",
    "        latent = torch.cat((latent_mean.clone(), latent_logvar.clone()), dim=-1).squeeze()\n",
    "        ## assume always add non-linearity to latent\n",
    "        return F.relu(latent), hidden_state\n",
    "\n",
    "class dummy_agent2:\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic\n",
    "\n",
    "    def act(self, state, latent, belief, task, deterministic = False):\n",
    "        return self.actor_critic.act(state, latent, belief, task, deterministic)\n",
    "    \n",
    "    def get_value(self, state, latent, belief, task):\n",
    "        return self.actor_critic.get_value(state, latent, belief, task)\n",
    "    \n",
    "    def get_latent(self, action, state, reward, hidden_state, return_prior = False):\n",
    "        _, latent_mean, latent_logvar, hidden_state = self.actor_critic.encoder(action, state, reward, hidden_state, return_prior = return_prior)\n",
    "        latent = torch.cat((latent_mean.clone(), latent_logvar.clone()), dim = -1)\n",
    "        ## assume always add non-linearity to latent\n",
    "        return F.relu(latent[None,:]), hidden_state\n",
    "    \n",
    "    def get_prior(self, num_processes):\n",
    "        _, latent_mean, latent_logvar, hidden_state = self.actor_critic.encoder.prior(num_processes)\n",
    "        latent = torch.cat((latent_mean.clone(), latent_logvar.clone()), dim=-1)\n",
    "        ## assume always add non-linearity to latent\n",
    "        return F.relu(latent), hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 40]) torch.Size([2, 1]) [False False] ({'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.01785945696830044, 'in_place_reward': 0.16334190586893652, 'obj_to_target': 0.2035419559095121, 'unscaled_reward': 0.03571891393660088, 'seq_idx': 0, 'env_name': 'push-v2', 'env': '<metaworld.envs.mujoco.sawyer_xyz.v2.sawyer_push_v2.SawyerPushEnvV2 object at 0x0000021AE2F8F2B0>'}, {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.01785945696830044, 'in_place_reward': 0.16334190586893652, 'obj_to_target': 0.2035419559095121, 'unscaled_reward': 0.03571891393660088, 'seq_idx': 0, 'env_name': 'push-v2', 'env': '<metaworld.envs.mujoco.sawyer_xyz.v2.sawyer_push_v2.SawyerPushEnvV2 object at 0x0000020DD986F2B0>'}) torch.Size([2, 4])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(done) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28many\u001b[39m(done), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetaworld envs should all end simultaneously\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_obs\u001b[38;5;241m.\u001b[39msize(), rew_raw\u001b[38;5;241m.\u001b[39msize(), done, info, action\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# create mask for episode ends\u001b[39;00m\n\u001b[0;32m     36\u001b[0m masks_done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m _done \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _done \u001b[38;5;129;01min\u001b[39;00m done])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RUN_FOLDER = './rl2_baseline/rl2_double_baseline'\n",
    "policy_net_agent = torch.load(RUN_FOLDER + '/models/policy.pt', map_location=device)\n",
    "encoder_net_agent = torch.load(RUN_FOLDER + '/models/encoder.pt', map_location=device)\n",
    "actor_critic = ActorCritic(policy_net_agent, encoder_net_agent)\n",
    "agent = dummy_agent1(actor_critic)\n",
    "eps = 0\n",
    "episode_reward=[]\n",
    "episode_norm_reward=[]\n",
    "\n",
    "# steps limit is parameter for whole continual env\n",
    "while envs.get_env_attr('cur_step') < envs.get_env_attr('steps_limit'):\n",
    "\n",
    "    step = 0\n",
    "    obs = envs.reset() # we reset all at once as metaworld is time limited\n",
    "    current_task = envs.get_env_attr(\"cur_seq_idx\")\n",
    "    episode_reward = []\n",
    "    successes = []\n",
    "    gating_values = []\n",
    "    done = [False for _ in range(num_processes)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        latent, hidden_state = agent.get_prior(num_processes)\n",
    "        # print(latent.size(), hidden_state.size())\n",
    "\n",
    "    while not all(done):\n",
    "        with torch.no_grad():\n",
    "            value, action = agent.act(None, latent, None, None, deterministic=True)\n",
    "\n",
    "        next_obs, (rew_raw, rew_normalised), done, info = envs.step(action.squeeze(0))\n",
    "        assert all(done) == any(done), \"Metaworld envs should all end simultaneously\"\n",
    "        print(next_obs.size(), rew_raw.size(), done, info, action.size())\n",
    "        raise ValueError\n",
    "\n",
    "        # create mask for episode ends\n",
    "        masks_done = torch.FloatTensor([[0.0] if _done else [1.0] for _done in done]).to(device)\n",
    "\n",
    "        ## combine all rewards\n",
    "        episode_reward.append(rew_raw)\n",
    "        episode_norm_reward.append(rew_normalised)\n",
    "        # if we succeed at all then the task is successful\n",
    "        successes.append(torch.tensor([i['success'] for i in info]))\n",
    "\n",
    "        latent, hidden_state = agent.get_latent(\n",
    "            action, next_obs, rew_raw, hidden_state, return_prior = False\n",
    "        )\n",
    "        # _, mu, logvar, hidden_state = (\n",
    "        #     agent\n",
    "        #     .actor_critic\n",
    "        #     .encoder(action, next_obs, rew_raw, hidden_state, return_prior=False)\n",
    "        # )\n",
    "        # latent = F.relu(torch.cat((mu, logvar), dim = -1).squeeze())\n",
    "            \n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        step += 1\n",
    "    eps+=1\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0368)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(episode_reward).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(successes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "## try creating a base env\n",
    "env_name='pick-place-v2'\n",
    "ml3 = ML3()\n",
    "envs = ml3.test_classes[env_name]()\n",
    "task = random.choice([task for task in ml3.test_tasks if task.env_name==env_name])\n",
    "envs.set_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\61417\\AppData\\Local\\Temp\\ipykernel_93256\\436205180.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(action).float().unsqueeze(0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2656)\n",
      "tensor(0.0224, dtype=torch.float64)\n",
      "tensor(0.0783)\n",
      "tensor(0.0195, dtype=torch.float64)\n",
      "tensor(0.0249)\n",
      "tensor(0.0410)\n",
      "tensor(0.0130)\n",
      "tensor(0.0562, dtype=torch.float64)\n",
      "tensor(0.0373)\n",
      "tensor(0.0290, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "RUN_FOLDER = './rl2_baseline/rl2_double_baseline'\n",
    "policy_net_agent = torch.load(RUN_FOLDER + '/models/policy.pt', map_location=device)\n",
    "encoder_net_agent = torch.load(RUN_FOLDER + '/models/encoder.pt', map_location=device)\n",
    "actor_critic = ActorCritic(policy_net_agent, encoder_net_agent)\n",
    "agent = dummy_agent1(actor_critic)\n",
    "for i in range(10):\n",
    "    episode_reward=[]\n",
    "\n",
    "    step = 0\n",
    "    obs = envs.reset() # we reset all at once as metaworld is time limited\n",
    "    episode_reward = []\n",
    "    successes = []\n",
    "    done=False\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        latent, hidden_state = agent.get_prior(1)\n",
    "        # print(latent.size(), hidden_state.size())\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            value, action = agent.act(None, latent, None, None, deterministic=False)\n",
    "\n",
    "        next_obs, reward,truncated, terminated, info = envs.step(action.numpy())\n",
    "        done = truncated or terminated\n",
    "        next_obs = np.concatenate((next_obs,[0.0]))\n",
    "\n",
    "        # create mask for episode ends\n",
    "        # masks_done = torch.FloatTensor([[0.0] if _done else [1.0] for _done in done]).to(device)\n",
    "\n",
    "        ## combine all rewards\n",
    "        episode_reward.append(reward)\n",
    "        # if we succeed at all then the task is successful\n",
    "        successes.append(info['success'])\n",
    "\n",
    "        latent, hidden_state = agent.get_latent(\n",
    "            torch.tensor(action).float().unsqueeze(0), \n",
    "            torch.tensor(next_obs).float().unsqueeze(0), \n",
    "            torch.tensor(reward).float().unsqueeze(0).unsqueeze(0), \n",
    "            hidden_state, \n",
    "            return_prior = False\n",
    "        )\n",
    "            \n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        step += 1\n",
    "    print(torch.tensor(episode_reward).mean())\n",
    "\n",
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.2883)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(episode_reward).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 40]) torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\61417\\AppData\\Local\\Temp\\ipykernel_93256\\26570706.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(action).float().unsqueeze(0).size(),\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    torch.tensor(action).float().unsqueeze(0).size(), \n",
    "    torch.tensor(next_obs).float().unsqueeze(0).size(), \n",
    "    torch.tensor(reward).float().unsqueeze(0).unsqueeze(0).size()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.24888659e-03,  5.99651668e-01,  1.94001019e-01,  1.00000000e+00,\n",
       "        4.65829553e-02,  6.01204335e-01,  1.99124146e-02, -1.95931737e-04,\n",
       "       -2.96176911e-04, -1.62033349e-08,  9.99999937e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  6.15235164e-03,  6.00189803e-01,\n",
       "        1.94301175e-01,  1.00000000e+00,  4.65942126e-02,  6.01196870e-01,\n",
       "        1.99999996e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((next_obs,[0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reach-v2'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice([task for task in ml3.train_classes.keys() if task == 'reach-v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import metaworld\n",
    "import random\n",
    "\n",
    "from environments.custom_metaworld_benchmark  import ML3\n",
    "\n",
    "class ML3SingleEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, env_name, train=False):\n",
    "        self.benchmark= ML3()\n",
    "        self.train = train\n",
    "        self.env_name = env_name\n",
    "        if train:\n",
    "            self.tasks = [task for task in self.benchmark.train_tasks if task.env_name==self.env_name]\n",
    "            self.env_cls = self.benchmark.train_classes[self.env_name]\n",
    "        else:\n",
    "            self.tasks = [task for task in self.benchmark.test_tasks if task.env_name==self.env_name]\n",
    "            self.env_cls = self.benchmark.test_classes[self.env_name]\n",
    "\n",
    "        ## set the task\n",
    "        self.env = self.env_cls()\n",
    "        self.task = random.choice(self.tasks)\n",
    "        self.env.set_task(self.task)\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "        # metaworld max steps - hardcoded\n",
    "        self._max_episode_steps = 500\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        info['task'] = self.task\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        obs, _ = self.env.reset()\n",
    "        return obs\n",
    "    \n",
    "    def get_task(self):\n",
    "        return self.task\n",
    "    \n",
    "    ## reset_task is automatically created in make_env using set_task\n",
    "    def set_task(self, task = None):\n",
    "        if task is None:\n",
    "            if self.train:\n",
    "                task = random.choice(\n",
    "                    [task for task in self.benchmark.train_tasks if task.env_name==self.env_name]\n",
    "                    )\n",
    "            else: \n",
    "                task = random.choice(\n",
    "                    [task for task in self.benchmark.test_tasks if task.env_name==self.env_name]\n",
    "                    )\n",
    "\n",
    "        self.task = task\n",
    "        self.env.set_task(self.task)\n",
    "\n",
    "    # duplicated for varibad temporarily\n",
    "    def reset_task(self, task = None):\n",
    "        if task is None:\n",
    "            if self.train:\n",
    "                task = random.choice(\n",
    "                    [task for task in self.benchmark.train_tasks if task.env_name==self.env_name]\n",
    "                    )\n",
    "            else: \n",
    "                task = random.choice(\n",
    "                    [task for task in self.benchmark.test_tasks if task.env_name==self.env_name]\n",
    "                    )\n",
    "\n",
    "        self.task = task\n",
    "        self.env.set_task(self.task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ml3_reach = ML3SingleEnv('reach-v2', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task(env_name='reach-v2', data=b'\\x80\\x04\\x954\\x01\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x08rand_vec\\x94\\x8c\\x15numpy.core.multiarray\\x94\\x8c\\x0c_reconstruct\\x94\\x93\\x94\\x8c\\x05numpy\\x94\\x8c\\x07ndarray\\x94\\x93\\x94K\\x00\\x85\\x94C\\x01b\\x94\\x87\\x94R\\x94(K\\x01K\\x06\\x85\\x94h\\x05\\x8c\\x05dtype\\x94\\x93\\x94\\x8c\\x02f8\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89C0\\xba\\xf1\\xc6\\xc4}L\\xb7?\\x0f\\xbdUV\\x05f\\xe5?\\x00\\x00\\x00@\\xe1z\\x94?dM/\\x18~\\xb5\\xa1?\\x9a\\xf4\\x93(Mf\\xeb?\\xe0b\\xebH<\\x96\\xce?\\x94t\\x94b\\x8c\\x07env_cls\\x94\\x8c3metaworld.envs.mujoco.sawyer_xyz.v2.sawyer_reach_v2\\x94\\x8c\\x10SawyerReachEnvV2\\x94\\x93\\x94\\x8c\\x14partially_observable\\x94\\x89u.')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ml3_reach.get_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from environments.parallel_envs import make_vec_envs, make_env\n",
    "from environments.env_utils.running_mean_std import RunningMeanStd\n",
    "\n",
    "# from models import encoder, policy\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_id, seed, rank, episodes_per_task, tasks, add_done_info, **kwargs\n",
    "env_fn = [make_env(\n",
    "    env_id='ML_3_single-v2',\n",
    "    seed=73,\n",
    "    rank=0+i,\n",
    "    episodes_per_task=1,\n",
    "    tasks=None,\n",
    "    add_done_info=True,\n",
    "    **{'task_name':'reach-v2', 'train':True}\n",
    "    ) for i in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.parallel_envs import SubprocVecEnv\n",
    "\n",
    "envs = SubprocVecEnv(env_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00615235,  0.6001898 ,  0.19430117,  1.        , -0.02596853,\n",
       "        0.68842153,  0.02      ,  0.        ,  0.        ,  0.        ,\n",
       "        1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00615235,  0.6001898 ,\n",
       "        0.19430117,  1.        , -0.02596853,  0.68842153,  0.02      ,\n",
       "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REW: tensor([[0.0141],\n",
      "        [0.0207],\n",
      "        [0.0176],\n",
      "        [0.0136],\n",
      "        [0.2409]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n",
      "REW: tensor([[1.6174],\n",
      "        [0.0083],\n",
      "        [0.0157],\n",
      "        [5.6966],\n",
      "        [3.5217]])\n",
      "SUCC: tensor([1., 0., 0., 1., 1.])\n",
      "===============================================\n",
      "REW: tensor([[0.0202],\n",
      "        [0.0129],\n",
      "        [0.1073],\n",
      "        [0.0165],\n",
      "        [0.0139]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n",
      "REW: tensor([[0.0461],\n",
      "        [0.0147],\n",
      "        [0.0148],\n",
      "        [0.0150],\n",
      "        [0.0148]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n",
      "REW: tensor([[0.0142],\n",
      "        [0.0144],\n",
      "        [0.0385],\n",
      "        [0.0182],\n",
      "        [0.0163]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n",
      "REW: tensor([[0.0156],\n",
      "        [0.0151],\n",
      "        [0.2548],\n",
      "        [0.0350],\n",
      "        [0.0187]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n",
      "REW: tensor([[0.0357],\n",
      "        [0.0313],\n",
      "        [0.0153],\n",
      "        [0.0258],\n",
      "        [0.0143]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n",
      "REW: tensor([[0.0289],\n",
      "        [0.0118],\n",
      "        [0.0138],\n",
      "        [0.0179],\n",
      "        [0.0139]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n",
      "REW: tensor([[0.0631],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0438],\n",
      "        [0.0148]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n",
      "REW: tensor([[0.0086],\n",
      "        [0.0075],\n",
      "        [0.0112],\n",
      "        [0.0151],\n",
      "        [0.0150]])\n",
      "SUCC: tensor([0., 0., 0., 0., 0.])\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "from algorithms.custom_ppo import CustomPPO\n",
    "from models.combined_actor_critic import ActorCritic\n",
    "\n",
    "num_processes=5\n",
    "envs = make_vec_envs(\n",
    "    env_name='ML_3_single-v2',\n",
    "    seed=73, \n",
    "    num_processes=num_processes, \n",
    "    gamma=0.99,\n",
    "    device=device, \n",
    "    episodes_per_task=1,\n",
    "    normalise_rew=True, \n",
    "    ret_rms=None, \n",
    "    tasks=None,\n",
    "    rank_offset=0,\n",
    "    add_done_info=True,\n",
    "    **{'task_name':'pick-place-v2', 'train':False}\n",
    ")\n",
    "\n",
    "RUN_FOLDER = './rl2_baseline/rl2_double_baseline'\n",
    "policy_net_agent = torch.load(RUN_FOLDER + '/models/policy.pt', map_location=device)\n",
    "encoder_net_agent = torch.load(RUN_FOLDER + '/models/encoder.pt', map_location=device)\n",
    "actor_critic = ActorCritic(policy_net_agent, encoder_net_agent)\n",
    "agent = dummy_agent1(actor_critic)\n",
    "for i in range(10):\n",
    "    episode_reward=[]\n",
    "\n",
    "    step = 0\n",
    "    obs = envs.reset() # we reset all at once as metaworld is time limited\n",
    "    episode_reward = []\n",
    "    successes = []\n",
    "    done=[False for _ in range(num_processes)]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        latent, hidden_state = agent.get_prior(num_processes)\n",
    "\n",
    "    while not all(done):\n",
    "        with torch.no_grad():\n",
    "            value, action = agent.act(None, latent, None, None, deterministic=False)\n",
    "\n",
    "        next_obs, (rew_raw, rew_normalised),done, info = envs.step(action)\n",
    "\n",
    "\n",
    "        # create mask for episode ends\n",
    "        # masks_done = torch.FloatTensor([[0.0] if _done else [1.0] for _done in done]).to(device)\n",
    "\n",
    "        ## combine all rewards\n",
    "        episode_reward.append(rew_raw)\n",
    "        # if we succeed at all then the task is successful\n",
    "        successes.append(torch.tensor([i['success'] for i in info]))\n",
    "\n",
    "        latent, hidden_state = agent.get_latent(\n",
    "            action, \n",
    "            next_obs, \n",
    "            rew_raw, \n",
    "            hidden_state, \n",
    "            return_prior = False\n",
    "        )\n",
    "            \n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        step += 1\n",
    "    print(\"REW:\", torch.stack(episode_reward).mean(0))\n",
    "    print(\"SUCC:\", torch.stack(successes).max(0)[0])\n",
    "    print(\"===============================================\")\n",
    "\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0152],\n",
       "        [2.6538],\n",
       "        [0.0139],\n",
       "        [0.3873],\n",
       "        [0.0132]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(episode_reward).mean(0)\n",
    "torch.stack(successes).max(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(successes).max(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(223.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(successes).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6111)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(episode_reward).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varibad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
