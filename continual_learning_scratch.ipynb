{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch development for continual learning loop\n",
    "\n",
    "What I want to do:\n",
    "- There is a continual learning env that lets an agent learn on a stream of tasks\n",
    "- At specified intervals, we can evaluate the continual learner on _every_ task in the training set\n",
    "\n",
    "Things to consider:\n",
    "- can I use the ML10 environments in this setting? goals are obscured, what does that mean?\n",
    "- how many steps per env do I want? I think CW uses 1 million? check this, also, with PPO is this enough? (CW uses SAC)\n",
    "- randomisation of task goals - CW seems to have randomisation settings for the benchmark. MT and ML have randomised environments. How should we handle these tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import metaworld\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### also some wrappers used \n",
    "# pop successes records the successes during training\n",
    "# randomisation settings - seems that they set all tasks at the start\n",
    "class ContinualEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Based on continual world env design:\n",
    "    https://github.com/awarelab/continual_world/blob/main/continualworld/envs.py\n",
    "    \"\"\"\n",
    "    def __init__(self, envs: List[gym.Env], steps_per_env: int):\n",
    "\n",
    "        ## good check to do\n",
    "        for i in range(len(envs)):\n",
    "            assert envs[0].action_space == envs[i].action_space\n",
    "\n",
    "        self.action_space = envs[0].action_space\n",
    "        self.observation_space = deepcopy(envs[0].observation_space)\n",
    "        # what is remove goal bounds? don't think need for meta-learning\n",
    "\n",
    "        self.envs = envs\n",
    "        self.num_envs = len(envs)\n",
    "        self.steps_per_env = steps_per_env\n",
    "        self.steps_limit = self.num_envs * self.steps_per_env\n",
    "        self.cur_step = 0\n",
    "        self.cur_seq_idx = 0\n",
    "\n",
    "    def _get_envs(self):\n",
    "        return self.envs\n",
    "\n",
    "    def step(self, action: Any) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        obs, reward, terminated, truncated, info = self.envs[self.cur_seq_idx].step(action)\n",
    "        done = terminated or truncated\n",
    "        info[\"seq_idx\"] = self.cur_seq_idx\n",
    "\n",
    "        self.cur_step += 1\n",
    "        if self.cur_step % self.steps_per_env == 0:\n",
    "            done = True\n",
    "            info[\"TimeLimit.truncated\"] = True\n",
    "\n",
    "            self.cur_seq_idx += 1\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        obs, _, self.envs[self.cur_seq_idx].reset()\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaworld\n",
    "import random\n",
    "\n",
    "ml10 = metaworld.ML10() # Construct the benchmark, sampling tasks\n",
    "\n",
    "training_envs = []\n",
    "for name, env_cls in ml10.test_classes.items():\n",
    "  env = env_cls()\n",
    "  task = random.choice([task for task in ml10.test_tasks\n",
    "                        if task.env_name == name])\n",
    "  env.set_task(task)\n",
    "  training_envs.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00615235, 0.60018983, 0.19430118, 1.        , 0.0357309 ,\n",
       "        0.72999998, 0.09      , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00615235, 0.60018983,\n",
       "        0.19430118, 1.        , 0.0357309 , 0.72999998, 0.09      ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " {})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_envs[0].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evalute_envs(envs_to_do, eval_episodes = 10, agent = None):\n",
    "\n",
    "    ## TODO: find a way to insert the agent\n",
    "\n",
    "    for env in envs_to_do:\n",
    "\n",
    "        for episode in range(eval_episodes):\n",
    "            done = False\n",
    "            episode_count = 0\n",
    "            episode_reward = 0\n",
    "            success = 0\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                action = env.action_space.sample()\n",
    "                next_obs, reward, done, info = cont_env.step(action)\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_env(env, num_episodes):\n",
    "    results = {i:{'episode_reward': 0, 'success': 0} for i in range(num_episodes)}\n",
    "    for episode in range(num_episodes):\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        success = 0\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, truncated, terminated, info = env.step(action)\n",
    "            done = truncated or terminated\n",
    "            obs = next_obs\n",
    "\n",
    "            episode_reward += reward\n",
    "            success = info['success']\n",
    "\n",
    "            # stop on success for eval\n",
    "            if success == 1:\n",
    "                done = True\n",
    "                \n",
    "        results[episode]['episode_reward'] = episode_reward\n",
    "        results[episode]['success'] = success\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1\n",
      "20 2\n",
      "30 3\n",
      "40 4\n",
      "50 5\n"
     ]
    }
   ],
   "source": [
    "## training loop\n",
    "# while the whole continual env is not done\n",
    "# train on each env sequentially\n",
    "# periodically evaluate on all envs\n",
    "cont_env = ContinualEnv(training_envs, 10)\n",
    "num_steps = 0\n",
    "while cont_env.cur_step < cont_env.steps_limit:\n",
    "    # do each env\n",
    "    obs = cont_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = cont_env.action_space.sample()\n",
    "        next_obs, reward, done, info = cont_env.step(action)\n",
    "        obs = next_obs\n",
    "        \n",
    "    print(cont_env.cur_step, cont_env.cur_seq_idx)\n",
    "\n",
    "    # periodically evaluate\n",
    "    all_envs = cont_env._get_envs()\n",
    "    eval_results = evaluate_envs(all_envs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (4,), float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = training_envs[0]\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.71689176e-03,  6.00775945e-01,  1.94656428e-01,  1.00000000e+00,\n",
       "         2.16917725e-02,  6.75859067e-01,  1.99142488e-02,  3.53522718e-04,\n",
       "         1.89404037e-04,  2.59762152e-09,  9.99999920e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  6.15235164e-03,  6.00189803e-01,\n",
       "         1.94301175e-01,  1.00000000e+00,  2.16845459e-02,  6.75872549e-01,\n",
       "         1.99999996e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -7.60647727e-02,  8.09090702e-01,  2.14756129e-01]),\n",
       " 1.4191217005275105,\n",
       " False,\n",
       " False,\n",
       " {'success': 0.0,\n",
       "  'near_object': 0.2327727045644655,\n",
       "  'grasp_success': 1.0,\n",
       "  'grasp_reward': 0.2327727045644655,\n",
       "  'in_place_reward': 0.14191217005275106,\n",
       "  'obj_to_target': 0.2327727045644655,\n",
       "  'unscaled_reward': 1.4191217005275105})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "action = env.action_space.sample()\n",
    "env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varibad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
