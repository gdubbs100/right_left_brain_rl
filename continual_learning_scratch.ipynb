{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch development for continual learning loop\n",
    "\n",
    "What I want to do:\n",
    "- There is a continual learning env that lets an agent learn on a stream of tasks\n",
    "- At specified intervals, we can evaluate the continual learner on _every_ task in the training set\n",
    "\n",
    "Things to consider:\n",
    "- can I use the ML10 environments in this setting? goals are obscured, what does that mean?\n",
    "- how many steps per env do I want? I think CW uses 1 million? check this, also, with PPO is this enough? (CW uses SAC)\n",
    "- randomisation of task goals - CW seems to have randomisation settings for the benchmark. MT and ML have randomised environments. How should we handle these tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other thoughts\n",
    "Next step is to integrate the continual environment and evaluation loop into a training loop. We want to use the PPO algorithm provided (or something similar), but provide it with arbitrary policy / value networks (i.e. our own).\n",
    "\n",
    "Testing learning in this way might require setting up some command line 'args' stuff - might be easier to match it with PPO\n",
    "But is this the simplest testing method?\n",
    "\n",
    "Should look at policy storage - how can I use that?\n",
    "\n",
    "Also - continue to look at how to get continual environments to work. Continual world has randomization handlers, success counters etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import metaworld\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.custom_ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./algorithms/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. get prior at start for base latent\n",
    "# (does this reset the hidden state? I think so)\n",
    "#2. feed policy observation + latent -> gets action, obs, reward, done\n",
    "#3. feed encoder action, obs, reward, done and hidden state to get next action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# evaluation loop across all environments\n",
    "for env_name, env, in environments.items():\n",
    "\n",
    "    # reset encoder to prior\n",
    "    latent, hidden_state = self.get_prior_latent()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        success = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            _, act = self.policy.act(obs, latent, None, None)\n",
    "            action = act.cpu().detach().numpy()[0]\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            obs = next_obs\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            success = 1 if info['success']==1 else 0\n",
    "\n",
    "            latent, hidden_state = self.update_latent(obs, act, reward, hidden_state)\n",
    "\n",
    "            # break loop during eval if task success\n",
    "            if success == 1:\n",
    "                done = True\n",
    "\n",
    "        episode_log[env_name]['episode_reward'].append(episode_reward / episode_steps)\n",
    "        episode_log[env_name]['episode_len'].append(episode_steps)\n",
    "        episode_log[env_name]['successes'].append(success)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### also some wrappers used \n",
    "# pop successes records the successes during training\n",
    "# randomisation settings - seems that they set all tasks at the start\n",
    "class ContinualEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Based on continual world env design:\n",
    "    https://github.com/awarelab/continual_world/blob/main/continualworld/envs.py\n",
    "    \"\"\"\n",
    "    def __init__(self, envs: List[gym.Env], steps_per_env: int):\n",
    "\n",
    "        ## good check to do\n",
    "        for i in range(len(envs)):\n",
    "            assert envs[0].action_space == envs[i].action_space\n",
    "\n",
    "        self.action_space = envs[0].action_space\n",
    "        self.observation_space = deepcopy(envs[0].observation_space)\n",
    "        # what is remove goal bounds? don't think need for meta-learning\n",
    "\n",
    "        self.envs = envs\n",
    "        self.num_envs = len(envs)\n",
    "        self.steps_per_env = steps_per_env\n",
    "        self.steps_limit = self.num_envs * self.steps_per_env\n",
    "        self.cur_step = 0\n",
    "        self.cur_seq_idx = 0\n",
    "\n",
    "    def _get_envs(self):\n",
    "        return self.envs\n",
    "\n",
    "    def step(self, action: Any) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        obs, reward, terminated, truncated, info = self.envs[self.cur_seq_idx].step(action)\n",
    "        done = terminated or truncated\n",
    "        info[\"seq_idx\"] = self.cur_seq_idx\n",
    "\n",
    "        self.cur_step += 1\n",
    "        if self.cur_step % self.steps_per_env == 0:\n",
    "            done = True\n",
    "            info[\"TimeLimit.truncated\"] = True\n",
    "\n",
    "            self.cur_seq_idx += 1\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        obs, _, self.envs[self.cur_seq_idx].reset()\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaworld\n",
    "import random\n",
    "\n",
    "ml10 = metaworld.ML10() # Construct the benchmark, sampling tasks\n",
    "\n",
    "training_envs = []\n",
    "for name, env_cls in ml10.test_classes.items():\n",
    "  env = env_cls()\n",
    "  task = random.choice([task for task in ml10.test_tasks\n",
    "                        if task.env_name == name])\n",
    "  env.set_task(task)\n",
    "  training_envs.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_ppo_acktr.algo import ppo\n",
    "from a2c_ppo_acktr import storage, model\n",
    "\n",
    "# get RL2 trained policy for example\n",
    "RUN_FOLDER = './logs/logs_ML10-v2/rl2_73__25:10_21:13:08'\n",
    "\n",
    "encoder_net = torch.load(RUN_FOLDER + '/models/encoder.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for dimension 0 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/grant/working_repos/varibad/continual_learning_scratch.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_learning_scratch.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_learning_scratch.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_learning_scratch.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39m# this might be funny\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_learning_scratch.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m         value, action, action_log_prob, recurrent_hidden_states \u001b[39m=\u001b[39m actor_critic\u001b[39m.\u001b[39mact(rollouts\u001b[39m.\u001b[39;49mobs[step], rollouts\u001b[39m.\u001b[39mrecurrent_hidden_states[step], rollouts\u001b[39m.\u001b[39mmasks[step], \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_learning_scratch.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m         step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_learning_scratch.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# step the env\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for dimension 0 with size 11"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# example training\n",
    "### set the environment\n",
    "env = training_envs[0]\n",
    "### create a policy / actor critic\n",
    "# policy_net = torch.load(RUN_FOLDER + '/models/policy.pt')\n",
    "# we can update this\n",
    "actor_critic = model.Policy(\n",
    "    env.observation_space.shape,\n",
    "    env.action_space,\n",
    "    base_kwargs={'recurrent': True})\n",
    "actor_critic.to(device)\n",
    "\n",
    "\n",
    "### set algorithm\n",
    "ppo_agent = ppo.PPO(\n",
    "    actor_critic = actor_critic,\n",
    "    clip_param = 0.2,\n",
    "    ppo_epoch = 4,\n",
    "    num_mini_batch = 10,\n",
    "    value_loss_coef = 0.5,\n",
    "    entropy_coef = 1.0e-5,\n",
    "    lr = 1.0e-4,\n",
    "    eps = 0.99,\n",
    "    max_grad_norm = 0.5,\n",
    "    use_clipped_value_loss = False\n",
    ")\n",
    "\n",
    "## create rollouts\n",
    "rollouts = storage.RolloutStorage(\n",
    "    10, 1, env.observation_space.shape, env.action_space, actor_critic.recurrent_hidden_state_size\n",
    ")\n",
    "\n",
    "# don't know\n",
    "obs, _ = env.reset()\n",
    "rollouts.obs[0].copy_(torch.from_numpy(obs))\n",
    "rollouts.to(device)\n",
    "episode_rewards = deque(maxlen = 10)\n",
    "\n",
    "# actual training loop\n",
    "start = time.time()\n",
    "num_updates = 10\n",
    "for j in range(num_updates):\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            # this might be funny\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(rollouts.obs[step], rollouts.recurrent_hidden_states[step], rollouts.masks[step], None)\n",
    "            step += 1\n",
    "\n",
    "        # step the env\n",
    "        obs, reward, truncated, terminated, info = env.step(action.detach().to('cpu').numpy()[0])\n",
    "        done = truncated or terminated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value\n",
    "action\n",
    "action_log_prob\n",
    "recurrent_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_envs(envs_to_do, num_episodes = 10, agent = None):\n",
    "\n",
    "    ## TODO: find a way to insert the agent\n",
    "    results = {i: dict() for i in range(len(envs_to_do))}\n",
    "    for i, env in enumerate(envs_to_do):\n",
    "        results[i] = evaluate_single_env(env, num_episodes)\n",
    "        _, _ = env.reset()\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_single_env(env, num_episodes):\n",
    "    results = {'episode_reward': [], 'success': []} \n",
    "    for episode in range(num_episodes):\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        success = 0\n",
    "        episode_len = 0\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, truncated, terminated, info = env.step(action)\n",
    "            done = truncated or terminated\n",
    "            obs = next_obs\n",
    "\n",
    "            episode_reward += reward\n",
    "            success = info['success']\n",
    "            episode_len += 1\n",
    "\n",
    "            # stop on success for eval\n",
    "            if success == 1:\n",
    "                done = True\n",
    "\n",
    "        results['episode_reward'].append(episode_reward / episode_len)\n",
    "        results['success'].append(success)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "# while the whole continual env is not done\n",
    "# train on each env sequentially\n",
    "# periodically evaluate on all envs\n",
    "cont_env = ContinualEnv(training_envs, 10)\n",
    "eval_freq = 10\n",
    "num_steps = 0\n",
    "eval_results = dict()\n",
    "while cont_env.cur_step < cont_env.steps_limit:\n",
    "    # do each env\n",
    "    obs = cont_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = cont_env.action_space.sample()\n",
    "        next_obs, reward, done, info = cont_env.step(action)\n",
    "        obs = next_obs\n",
    "\n",
    "        # periodically evaluate\n",
    "        if  (cont_env.cur_step + 1) % eval_freq == 0:\n",
    "            print(cont_env.cur_step, 'EVALUATING')\n",
    "            all_envs = cont_env._get_envs()\n",
    "            eval_results[cont_env.cur_step] = evaluate_all_envs(all_envs, num_episodes = 3)\n",
    "            eval_results[cont_env.cur_step]['task'] = cont_env.cur_seq_idx\n",
    "        \n",
    "    print(cont_env.cur_step, cont_env.cur_seq_idx)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_results(results_dict, _iter):\n",
    "    df = pd.DataFrame(results_dict[_iter])\n",
    "    task = np.unique(df.task).item()\n",
    "    res =df\\\n",
    "        .loc[:,[col for col in df.columns if col != 'task']]\\\n",
    "        .reset_index(\n",
    "            names = 'metric'\n",
    "        )\\\n",
    "        .melt(\n",
    "            id_vars = 'metric',\n",
    "            var_name = 'task'\n",
    "        )\\\n",
    "        .explode('value')\n",
    "    res.loc[:, 'iter'] = _iter\n",
    "    res.loc[:, 'current_task'] = task\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res = pd.concat([unpack_results(eval_results, i) for i in eval_results.keys()])\n",
    "\n",
    "successes = res\\\n",
    ".query('metric==\"success\"')\\\n",
    ".groupby(['task', 'iter'])\\\n",
    ".agg(\n",
    "    {\n",
    "        'current_task':'max',\n",
    "        'value': lambda x: sum(x) / len(x)\n",
    "    }\n",
    ")\\\n",
    ".reset_index()\n",
    "\n",
    "rewards = res.query('metric==\"episode_reward\"')\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (15, 7))\n",
    "\n",
    "sns.lineplot(\n",
    "    data = rewards,\n",
    "    x = 'iter',\n",
    "    y = 'value',\n",
    "    hue = 'task',\n",
    "    ax = ax[0]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data = successes,\n",
    "    x = 'iter',\n",
    "    y = 'value',\n",
    "    hue = 'task',\n",
    "    ax = ax[1]\n",
    ")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(eval_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(df.iloc[0, 0], max_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, [(i, pd.v) for i, v in v.items()]) for k, v in eval_results.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{{'iter': k, 'value': pd.json_normalize(v)} for k, v in eval_results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "action = env.action_space.sample()\n",
    "env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varibad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
