{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('./algorithms/')\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, policy, encoder):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def get_actor_params(self):\n",
    "        return self.policy.get_actor_params()\n",
    "\n",
    "    def get_critic_params(self):\n",
    "        return self.policy.get_critic_params()\n",
    "\n",
    "    def forward_actor(self, inputs):\n",
    "        return self.policy.forward_actor(inputs)\n",
    "\n",
    "    def forward_critic(self, inputs):\n",
    "        return self.policy.forward_critic(inputs)\n",
    "    \n",
    "    def act(self, state, latent, belief=None, task=None, deterministic = False):\n",
    "        return self.policy.act(state, latent, belief, task, deterministic)\n",
    "\n",
    "    def get_value(self, state, latent, belief=None, task=None):\n",
    "        value, _ = self.policy.forward(state, latent, belief, task)\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, state, latent, belief, task, action):\n",
    "        \"\"\"Call policy eval, set task, belief to None\"\"\"\n",
    "        return self.policy.evaluate_actions(state, latent, belief, task, action)\n",
    "    \n",
    "        ## TODO: what to do about 'sample'? check what this arg is?\n",
    "    # def forward(self, actions, states, rewards, hidden_state, return_prior=False, sample=True, detach_every=None):\n",
    "    #     # really want this to take the inputs for the encoder and then output the outputs of the policy\n",
    "    #     # we only want to get the prior when there are no previous rewards, actions or hidden states\n",
    "    #     # should only occur at the very start of the continual learning process\n",
    "    #     if hidden_state is None:\n",
    "    #         # print('Hidden state is None!!:', hidden_state)\n",
    "    #         _, latent_mean, latent_logvar, hidden_state = self.encoder.prior(states.shape[1]) # check that this gets the batch size?\n",
    "    #     else:\n",
    "    #         _, latent_mean, latent_logvar, hidden_state = self.encoder(actions, states, rewards, hidden_state, return_prior, sample, detach_every)\n",
    "        \n",
    "    #     latent_mean = F.relu(latent_mean)\n",
    "    #     latent_logvar = F.relu(latent_logvar)\n",
    "    #     latent = torch.cat((latent_mean, latent_logvar), dim=-1).reshape(1, -1)\n",
    "    #     # none for belief and task\n",
    "    #     return self.policy(states, latent, None, None), hidden_state, latent\n",
    "    \n",
    "    # def prior(self, num_processes):\n",
    "    #     return self.encoder.prior(num_processes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get RL2 trained policy for example\n",
    "RUN_FOLDER = './logs/logs_ML10-v2/rl2_73__25:10_21:13:08'\n",
    "policy_net = torch.load(RUN_FOLDER + '/models/policy.pt')\n",
    "encoder_net = torch.load(RUN_FOLDER + '/models/encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. get prior at start for base latent\n",
    "# (does this reset the hidden state? I think so)\n",
    "#2. feed policy observation + latent -> gets action, obs, reward, done\n",
    "#3. feed encoder action, obs, reward, done and hidden state to get next action\n",
    "import metaworld\n",
    "import random\n",
    "\n",
    "ml10 = metaworld.ML10() # Construct the benchmark, sampling tasks\n",
    "\n",
    "training_envs = []\n",
    "for name, env_cls in ml10.test_classes.items():\n",
    "  env = env_cls()\n",
    "  task = random.choice([task for task in ml10.test_tasks\n",
    "                        if task.env_name == name])\n",
    "  env.set_task(task)\n",
    "  training_envs.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.env_utils.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from environments.parallel_envs import VecPyTorch\n",
    "\n",
    "def make_continual_env(env_id, **kwargs):\n",
    "    def _thunk():\n",
    "\n",
    "        env = gym.make(env_id, **kwargs)\n",
    "        # if tasks is not None:\n",
    "        #     env.unwrapped.reset_task = lambda x: env.unwrapped.set_task(random.choice(tasks))\n",
    "        # if seed is not None:\n",
    "        #     env.seed(seed + rank)\n",
    "        # if str(env.__class__.__name__).find('TimeLimit') >= 0:\n",
    "        #     env = TimeLimitMask(env)\n",
    "        # env = VariBadWrapper(env=env, episodes_per_task=episodes_per_task, add_done_info=add_done_info)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "vec_envs = SubprocVecEnv([make_continual_env('continualMW-v0', **{'envs' : training_envs}) for i in range(4)])\n",
    "# pyt_vec = VecPyTorch(vec_envs, device)\n",
    "# pyt_vec2 = VecPyTorch([make_continual_env('continualMW-v0', **{'envs' : training_envs}) for i in range(4)], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from algorithms.custom_storage import CustomOnlineStorage\n",
    "from algorithms.custom_ppo import CustomPPO\n",
    "# from environments.metaworld_envs.test_continual_env import ContinualEnv\n",
    "\n",
    "num_processes = 4\n",
    "# combined network with encoder + policy\n",
    "ac = ActorCritic(policy_net, encoder_net)\n",
    "agent = CustomPPO(\n",
    "    actor_critic=ac,\n",
    "    value_loss_coef = 0.4,\n",
    "    entropy_coef = 0.001,\n",
    "    policy_optimiser='adam',\n",
    "    policy_anneal_lr=False,\n",
    "    train_steps = 2,\n",
    "    lr = 1.0e-5,\n",
    "    eps=1.0e-8,\n",
    "    clip_param = 0.2,\n",
    "    ppo_epoch = 3,\n",
    "    use_huber_loss = True,\n",
    "    use_clipped_value_loss=True,\n",
    "    context_window=None\n",
    ")\n",
    "# env = training_envs[0]\n",
    "# env = ContinualEnv(training_envs, 500)\n",
    "# envs = SubprocVecEnv([make_continual_env('continualMW-v0', **{'envs' : training_envs}) for i in range(num_processes)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: make sure this can get all other properties of envs\n",
    "from environments.env_utils.vec_env import VecEnvWrapper\n",
    "class PyTorchVecEnvCont(VecEnvWrapper):\n",
    "\n",
    "    def __init__(self, vec_envs, device):\n",
    "        super(PyTorchVecEnvCont, self).__init__(vec_envs)\n",
    "        self.device = device\n",
    "  \n",
    "    def step_async(self, actions):\n",
    "        # actions = actions.squeeze(1).cpu().numpy()\n",
    "        # convert actions for worker .permute(1, 0, 2)\n",
    "        actions = actions.squeeze().cpu().numpy()\n",
    "        self.venv.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        state, reward, done, info = self.venv.step_wait()\n",
    "        if isinstance(state, list):  # raw + normalised .permute(1, 0, 2)\n",
    "            state = [torch.from_numpy(s).float().to(self.device) for s in state]\n",
    "        else:\n",
    "            state = torch.from_numpy(state).permute(1, 0, 2).float().to(self.device)\n",
    "        # reshape rewards to have dim T X B X D .reshape(1, -1, 1)\n",
    "        if isinstance(reward, list):  # raw + normalised\n",
    "            reward = [torch.from_numpy(r).unsqueeze(dim=1).reshape(1, -1, 1).float().to(self.device) for r in reward]\n",
    "        else:\n",
    "            reward = torch.from_numpy(reward).unsqueeze(dim=1).reshape(1, -1, 1).float().to(self.device)\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # if task is not None:\n",
    "        #     assert isinstance(task, list)\n",
    "        state = self.venv.reset()\n",
    "        ## permute state to have dimensions T X B X D .permute(1,0,2)\n",
    "        if isinstance(state, list):\n",
    "            state = [torch.from_numpy(s).float().to(self.device) for s in state]\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().to(self.device)\n",
    "        return state\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\" If env does not have the attribute then call the attribute in the wrapped_env \"\"\"\n",
    "\n",
    "        if attr in ['_max_episode_steps', 'task_dim', 'belief_dim', 'num_states']:\n",
    "            return self.unwrapped.get_env_attr(attr)\n",
    "\n",
    "        try:\n",
    "            orig_attr = self.__getattribute__(attr)\n",
    "        except AttributeError:\n",
    "            orig_attr = self.unwrapped.__getattribute__(attr)\n",
    "\n",
    "        if callable(orig_attr):\n",
    "            def hooked(*args, **kwargs):\n",
    "                result = orig_attr(*args, **kwargs)\n",
    "                return result\n",
    "\n",
    "            return hooked\n",
    "        else:\n",
    "            return orig_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step: 0; limit: 2500\n",
      "0\n",
      "current step: 500; limit: 2500\n",
      "0\n",
      "current step: 1000; limit: 2500\n",
      "0\n",
      "current step: 1500; limit: 2500\n",
      "0\n",
      "current step: 2000; limit: 2500\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# num_episodes_per_update = 4\n",
    "envs = SubprocVecEnv([make_continual_env('continualMW-v0', **{'envs' : training_envs, 'steps_per_env': 500}) for i in range(num_processes)])\n",
    "env = PyTorchVecEnvCont(envs, device)\n",
    "storage = CustomOnlineStorage(\n",
    "    500, num_processes, env.observation_space.shape[0]+1, 0, 0,\n",
    "    env.action_space, ac.encoder.hidden_size, ac.encoder.latent_dim, False)\n",
    "res = dict()\n",
    "while env.get_env_attr('cur_step') < env.get_env_attr('steps_limit'):\n",
    "    print(f\"current step: {env.get_env_attr('cur_step')}; limit: {env.get_env_attr('steps_limit')}\")\n",
    "    step = 0\n",
    "    eps = 0\n",
    "    \n",
    "    # if I do this, I need to make sure my returns are calculated correctly / the done flags work\n",
    "    # for i in range(num_episodes_per_update):\n",
    "    obs = env.reset()\n",
    "    done = [False for _ in range(num_processes)]\n",
    "    # print(f\"running episode {i}\")\n",
    "    ## get prior??? how frequent?\n",
    "    # do at start of each episode for now\n",
    "    with torch.no_grad():\n",
    "        _, latent_mean, latent_logvar, hidden_state = agent.actor_critic.encoder.prior(num_processes)\n",
    "        print(step)\n",
    "        ## TODO: set the 500 to some sort of variable (max episode len?)\n",
    "        assert len(storage.latent) == 0  # make sure we emptied buffers\n",
    "        # print(f\"saving hidden state to {i * 500}\")\n",
    "        storage.hidden_states[:1].copy_(hidden_state)\n",
    "        latent = torch.cat((latent_mean.clone(), latent_logvar.clone()), dim=-1)#.reshape(1, -1)\n",
    "        storage.latent.append(latent)\n",
    "\n",
    "    while not all(done):\n",
    "        value, action = agent.act(obs, latent, None, None)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        assert all(done) == any(done), \"Metaworld envs should all end simultaneously\"\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        ## TODO: do I even need masks?\n",
    "        # create mask for episode ends\n",
    "        masks_done = torch.FloatTensor([[0.0] if _done else [1.0] for _done in done]).to(device)\n",
    "        # bad_mask is true if episode ended because time limit was reached\n",
    "        # don't care for metaworld\n",
    "        bad_masks = torch.FloatTensor([[0.0] for _done in done]).to(device)\n",
    "\n",
    "        # if done:\n",
    "        #     print(f'{step}: done!')\n",
    "        #     hidden_state = agent.actor_critic.encoder.reset_hidden(hidden_state, masks_done)\n",
    "        # print(action.size(), obs.squeeze(0).size(), reward.squeeze(0).size(), hidden_state.size(), latent.size(), masks_done.size(), bad_masks.size())\n",
    "        _, latent_mean, latent_logvar, hidden_state = agent.actor_critic.encoder(action, obs, reward, hidden_state, return_prior = False)\n",
    "        latent = torch.cat((latent_mean.clone(), latent_logvar.clone()), dim = -1)[None,:]#.reshape(1, -1)\n",
    "\n",
    "        \n",
    "        storage.next_state[step] = obs.clone()\n",
    "        # print(action.squeeze(0).size(), obs.squeeze(0).size(), reward.squeeze(0).size(), hidden_state.size(), latent.size(), masks_done.size(), bad_masks.size())\n",
    "        storage.insert(\n",
    "            state=obs.squeeze(),\n",
    "            belief=None,\n",
    "            task=None,\n",
    "            actions=action.double(),\n",
    "            rewards_raw=reward.squeeze(0),\n",
    "            rewards_normalised=reward.squeeze(0),#rew_normalised,\n",
    "            value_preds=value.squeeze(0),\n",
    "            masks=masks_done.squeeze(0), # do I even need these?\n",
    "            bad_masks=bad_masks.squeeze(0), \n",
    "            done=torch.from_numpy(done)[:,None].float(),\n",
    "            hidden_states = hidden_state.squeeze(),\n",
    "            latent = latent#.unsqueeze(1),\n",
    "        )\n",
    "\n",
    "        step += 1\n",
    "        ### update\n",
    "        # if step % num_updates ==0:\n",
    "\n",
    "    # update at the end of each episode?\n",
    "    res[eps] = agent.update(storage)\n",
    "    # # # should clear out old data\n",
    "    storage.after_update()\n",
    "    eps+=1\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (1.437764875094096,\n",
       "  -0.0001040075007438195,\n",
       "  4.667186292012532,\n",
       "  0.570334771179074)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, log_probs, action_values = agent.actor_critic.evaluate_actions(\n",
    "    storage.prev_state[:-1],\n",
    "    torch.stack(storage.latent[:-1]),#torch.cat(storage.latent[:-1]),\n",
    "    None,\n",
    "    None,\n",
    "    storage.actions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 1, 4, 256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(storage.latent[:-1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 4, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(storage.latent[:-1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-185.0985, device='cuda:0', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 4, 256]) torch.Size([500, 4, 40]) torch.Size([500, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    torch.cat(storage.latent[:-1]).size(),\n",
    "    storage.prev_state[:-1].size(),\n",
    "    storage.actions.size()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3123, -0.8121,  0.7445,  1.1550],\n",
       "         [ 1.8017,  0.5680,  0.8695, -0.4735],\n",
       "         [ 1.4292,  1.8534, -1.0493,  1.1551],\n",
       "         [-0.4722, -0.7281,  0.4747,  0.6222]],\n",
       "\n",
       "        [[ 0.6972,  0.4875, -0.3274,  1.7530],\n",
       "         [ 0.2888,  0.4767,  0.5483,  0.1638],\n",
       "         [ 1.5668,  1.2545, -0.5558,  0.1735],\n",
       "         [ 2.8981,  0.4671,  0.6913, -0.2662]],\n",
       "\n",
       "        [[ 1.5318,  0.9153,  0.5392,  2.5992],\n",
       "         [ 0.7968,  1.8277, -1.0111, -0.1006],\n",
       "         [ 1.0232,  1.3677,  0.6708,  0.7862],\n",
       "         [ 1.1805,  0.6021, -0.8926, -0.9356]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-8.6578,  0.1204, -1.5586, -3.0550],\n",
       "         [-6.8673, -1.9840, -1.3414, -2.6791],\n",
       "         [-6.9235, -0.4342, -2.6258, -2.8675],\n",
       "         [-7.2971, -0.7323, -1.4768, -3.6448]],\n",
       "\n",
       "        [[-6.4243, -0.5177, -2.2652, -3.3827],\n",
       "         [-7.2827, -1.2557, -0.5568, -3.5874],\n",
       "         [-6.5266, -1.6540, -2.1240, -2.7305],\n",
       "         [-7.6278, -0.2197, -1.0824, -3.0855]],\n",
       "\n",
       "        [[-6.6662, -0.2482, -2.5053, -2.8690],\n",
       "         [-7.6460, -1.2500, -2.5183, -2.4874],\n",
       "         [-5.6179, -0.0805, -1.4439, -3.3891],\n",
       "         [-6.3951, -0.9108, -1.9805, -3.7339]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- get action values --\n",
    "advantages = storage.returns[:-1] - storage.value_preds[:-1]\n",
    "advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
    "\n",
    "# recompute embeddings (to build computation graph)\n",
    "agent._recompute_embeddings(storage, sample=False, update_idx=0,\n",
    "                    detach_every= agent.context_window if agent.context_window is not None else None)\n",
    "\n",
    "# update the normalisation parameters of policy inputs before updating\n",
    "# don't think I need this\n",
    "# self.actor_critic.update_rms(args=self.args, policy_storage=policy_storage)\n",
    "\n",
    "# call this to make sure that the action_log_probs are computed\n",
    "# (needs to be done right here because of some caching thing when normalising actions)\n",
    "storage.before_update(agent.actor_critic)\n",
    "\n",
    "value_loss_epoch = 0\n",
    "action_loss_epoch = 0\n",
    "dist_entropy_epoch = 0\n",
    "loss_epoch = 0\n",
    "for e in range(agent.ppo_epoch):\n",
    "\n",
    "    data_generator = storage.feed_forward_generator(advantages, 20)\n",
    "    for sample in data_generator:\n",
    "\n",
    "        state_batch, actions_batch, latent_batch, value_preds_batch, \\\n",
    "        return_batch, old_action_log_probs_batch, adv_targ = sample\n",
    "        break\n",
    "        # if not rlloss_through_encoder:\n",
    "        # state_batch = state_batch.detach()\n",
    "        ## TODO: I think I should not detach this\n",
    "        latent_batch = latent_batch#.detach()\n",
    "\n",
    "        # Reshape to do in a single forward pass for all steps\n",
    "        values, action_log_probs, dist_entropy = \\\n",
    "            agent.actor_critic.evaluate_actions(state=state_batch, latent=latent_batch,\n",
    "                                                belief=None, task=None,\n",
    "                                                action=actions_batch)\n",
    "        # break\n",
    "        ratio = torch.exp(action_log_probs.double() -\n",
    "                            old_action_log_probs_batch.double())\n",
    "        # break\n",
    "\n",
    "        surr1 = ratio * adv_targ\n",
    "        surr2 = torch.clamp(ratio, 1.0 - agent.clip_param, 1.0 + agent.clip_param) * adv_targ\n",
    "        action_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        if agent.use_huber_loss and agent.use_clipped_value_loss:\n",
    "            value_pred_clipped = value_preds_batch + (values - value_preds_batch).clamp(-agent.clip_param,\n",
    "                                                                                        agent.clip_param)\n",
    "            value_losses = F.smooth_l1_loss(values, return_batch, reduction='none')\n",
    "            value_losses_clipped = F.smooth_l1_loss(value_pred_clipped, return_batch, reduction='none')\n",
    "            value_loss = 0.5 * torch.max(value_losses, value_losses_clipped).mean()\n",
    "        elif agent.use_huber_loss:\n",
    "            value_loss = F.smooth_l1_loss(values, return_batch)\n",
    "        elif agent.use_clipped_value_loss:\n",
    "            value_pred_clipped = value_preds_batch + (values - value_preds_batch).clamp(-agent.clip_param,\n",
    "                                                                                        agent.clip_param)\n",
    "            value_losses = (values - return_batch).pow(2)\n",
    "            value_losses_clipped = (value_pred_clipped - return_batch).pow(2)\n",
    "            value_loss = 0.5 * torch.max(value_losses, value_losses_clipped).mean()\n",
    "        else:\n",
    "            value_loss = 0.5 * (return_batch - values).pow(2).mean()\n",
    "\n",
    "        # zero out the gradients\n",
    "        agent.optimiser.zero_grad()\n",
    "\n",
    "        # compute policy loss and backprop\n",
    "        loss = value_loss * agent.value_loss_coef + action_loss - dist_entropy * agent.entropy_coef\n",
    "\n",
    "        # compute vae loss and backprop\n",
    "        # if rlloss_through_encoder:\n",
    "        #     loss += self.args.vae_loss_coeff * compute_vae_loss()\n",
    "\n",
    "        # compute gradients (will attach to all networks involved in this computation)\n",
    "        loss.backward()\n",
    "\n",
    "        # clip gradients\n",
    "        nn.utils.clip_grad_norm_(agent.actor_critic.parameters(), agent.max_grad_norm)\n",
    "        # nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.args.policy_max_grad_norm)\n",
    "\n",
    "        # in oursetup loss is always through encoder\n",
    "        # if rlloss_through_encoder:\n",
    "        #     if self.args.encoder_max_grad_norm is not None:\n",
    "        #         nn.utils.clip_grad_norm_(encoder.parameters(), self.args.encoder_max_grad_norm)\n",
    "\n",
    "\n",
    "\n",
    "        value_loss_epoch += value_loss.item()\n",
    "        action_loss_epoch += action_loss.item()\n",
    "        dist_entropy_epoch += dist_entropy.item()\n",
    "        loss_epoch += loss.item()\n",
    "        print(value_loss, action_loss, dist_entropy, loss_epoch)\n",
    "\n",
    "        # if rlloss_through_encoder:\n",
    "        # recompute embeddings (to build computation graph) during updates\n",
    "        agent._recompute_embeddings(storage, sample=False, update_idx=e + 1,\n",
    "                                        detach_every= agent.context_window if agent.context_window is not None else None)\n",
    "        # utl.recompute_embeddings(policy_storage, encoder, sample=False, update_idx=e + 1,\n",
    "        #                              detach_every= self.context_window if self.context_window is not None else None)\n",
    "\n",
    "\n",
    "\n",
    "# num_updates = agent.ppo_epoch * agent.num_mini_batch\n",
    "\n",
    "# value_loss_epoch /= num_updates\n",
    "# action_loss_epoch /= num_updates\n",
    "# dist_entropy_epoch /= num_updates\n",
    "# loss_epoch /= num_updates\n",
    "\n",
    "# print(value_loss_epoch, action_loss_epoch, dist_entropy_epoch, loss_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.1854],\n",
       "         [-4.4164],\n",
       "         [-4.4030],\n",
       "         [-4.5090]],\n",
       "\n",
       "        [[-4.4312],\n",
       "         [-3.3822],\n",
       "         [-6.0335],\n",
       "         [-5.9839]],\n",
       "\n",
       "        [[-4.7691],\n",
       "         [-4.0127],\n",
       "         [-3.6651],\n",
       "         [-3.7823]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.2119],\n",
       "         [-4.2209],\n",
       "         [-4.1228],\n",
       "         [-4.8274]],\n",
       "\n",
       "        [[-6.6564],\n",
       "         [-5.0212],\n",
       "         [-3.8304],\n",
       "         [-5.3056]],\n",
       "\n",
       "        [[-3.8214],\n",
       "         [-4.3288],\n",
       "         [-4.3263],\n",
       "         [-3.3277]]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.action_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.1908],\n",
       "        [ -3.5976],\n",
       "        [ -5.4055],\n",
       "        [ -6.2718],\n",
       "        [ -4.2716],\n",
       "        [ -4.6790],\n",
       "        [ -3.6692],\n",
       "        [ -4.5106],\n",
       "        [ -3.8674],\n",
       "        [ -4.1244],\n",
       "        [ -5.4279],\n",
       "        [ -3.8490],\n",
       "        [ -5.4406],\n",
       "        [ -4.5643],\n",
       "        [ -7.5824],\n",
       "        [ -3.2127],\n",
       "        [ -3.5732],\n",
       "        [ -3.7019],\n",
       "        [ -3.1522],\n",
       "        [ -5.0758],\n",
       "        [ -3.6826],\n",
       "        [ -4.6915],\n",
       "        [ -3.3817],\n",
       "        [-10.2276],\n",
       "        [ -3.4740],\n",
       "        [ -5.1020],\n",
       "        [ -3.7318],\n",
       "        [ -5.4021],\n",
       "        [ -6.7121],\n",
       "        [ -5.1902],\n",
       "        [ -5.2535],\n",
       "        [ -7.0028],\n",
       "        [ -5.0640],\n",
       "        [ -4.0627],\n",
       "        [ -2.9140],\n",
       "        [ -4.1290],\n",
       "        [ -4.5848],\n",
       "        [ -3.5823],\n",
       "        [ -3.7406],\n",
       "        [ -4.8732],\n",
       "        [ -5.6764],\n",
       "        [ -4.0928],\n",
       "        [ -7.2210],\n",
       "        [ -5.7211],\n",
       "        [ -3.5553],\n",
       "        [ -5.6675],\n",
       "        [ -3.2729],\n",
       "        [ -8.3437],\n",
       "        [ -3.5655],\n",
       "        [ -4.0110],\n",
       "        [ -3.4103],\n",
       "        [ -2.8091],\n",
       "        [ -3.0777],\n",
       "        [ -4.2653],\n",
       "        [ -5.9193],\n",
       "        [ -3.1436],\n",
       "        [ -3.3094],\n",
       "        [ -3.4693],\n",
       "        [ -4.0403],\n",
       "        [ -4.1132],\n",
       "        [ -4.6327],\n",
       "        [ -4.1451],\n",
       "        [ -7.2462],\n",
       "        [ -8.0181],\n",
       "        [ -3.1562],\n",
       "        [ -4.4798],\n",
       "        [ -4.1652],\n",
       "        [ -4.3628],\n",
       "        [ -5.8526],\n",
       "        [ -3.1713],\n",
       "        [ -2.9229],\n",
       "        [ -4.0814],\n",
       "        [ -4.4599],\n",
       "        [ -5.0253],\n",
       "        [ -4.8651],\n",
       "        [ -7.7165],\n",
       "        [ -5.6567],\n",
       "        [ -5.2363],\n",
       "        [ -4.2457],\n",
       "        [ -5.3344],\n",
       "        [ -3.5361],\n",
       "        [ -5.9531],\n",
       "        [ -4.0302],\n",
       "        [ -6.7374],\n",
       "        [ -3.4512],\n",
       "        [ -3.2755],\n",
       "        [ -4.3172],\n",
       "        [ -4.9588],\n",
       "        [ -2.9199],\n",
       "        [ -3.0064],\n",
       "        [ -2.9997],\n",
       "        [ -3.1654],\n",
       "        [ -8.3921],\n",
       "        [ -4.5693],\n",
       "        [ -3.8945],\n",
       "        [ -4.7170],\n",
       "        [ -3.0286],\n",
       "        [ -5.0911],\n",
       "        [ -4.1920],\n",
       "        [ -4.2363]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_action_log_probs_batch #action_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan], device='cuda:0', grad_fn=<SelectBackward>) tensor([nan], device='cuda:0') tensor([nan], device='cuda:0', dtype=torch.float64, grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(action_log_probs[1], old_action_log_probs_batch[1], torch.exp(action_log_probs[1].double()-old_action_log_probs_batch[1].double()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3499e+32],\n",
       "        [1.6528e+30],\n",
       "        [5.1500e+24],\n",
       "        [8.3923e+31],\n",
       "        [1.3204e+37],\n",
       "        [3.8692e+14],\n",
       "        [3.4331e+00],\n",
       "        [1.1165e+35],\n",
       "        [2.1644e+28],\n",
       "        [2.0399e+34],\n",
       "        [3.3670e+16],\n",
       "        [6.8058e+32],\n",
       "        [4.5027e+27],\n",
       "        [1.5816e+37],\n",
       "        [1.3747e+34],\n",
       "        [1.6915e+42],\n",
       "        [3.7975e+31],\n",
       "        [2.8370e+37],\n",
       "        [2.9846e+26],\n",
       "        [8.2372e+36],\n",
       "        [1.0000e+00],\n",
       "        [2.5020e+30],\n",
       "        [1.9451e+26],\n",
       "        [8.4889e+30],\n",
       "        [7.6723e+29],\n",
       "        [4.5660e+32],\n",
       "        [4.8400e+05],\n",
       "        [3.0735e+33],\n",
       "        [4.8087e+30],\n",
       "        [8.4137e+25],\n",
       "        [6.6569e+33],\n",
       "        [1.5967e+30],\n",
       "        [2.1457e+26],\n",
       "        [7.2418e+32],\n",
       "        [1.3342e+40],\n",
       "        [4.1398e+28],\n",
       "        [3.4154e+23],\n",
       "        [8.5343e+27],\n",
       "        [2.2317e+25],\n",
       "        [1.1792e+29],\n",
       "        [2.2029e+00],\n",
       "        [3.4505e+33],\n",
       "        [1.2725e+29],\n",
       "        [9.8799e+29],\n",
       "        [1.6061e+28],\n",
       "        [7.9238e+22],\n",
       "        [8.6552e+34],\n",
       "        [5.3899e+27],\n",
       "        [1.0447e+24],\n",
       "        [3.6828e+30],\n",
       "        [9.9118e+31],\n",
       "        [1.4432e+33],\n",
       "        [2.9298e+22],\n",
       "        [1.6664e+28],\n",
       "        [1.0274e+38],\n",
       "        [5.0192e+28],\n",
       "        [3.4257e+19],\n",
       "        [5.4179e+28],\n",
       "        [3.6520e+34],\n",
       "        [3.4236e+00],\n",
       "        [3.3420e+29],\n",
       "        [1.2036e+21],\n",
       "        [6.1931e+23],\n",
       "        [4.4815e+00],\n",
       "        [6.6747e+29],\n",
       "        [6.3990e+28],\n",
       "        [7.2509e+25],\n",
       "        [3.7962e+30],\n",
       "        [1.7419e+36],\n",
       "        [9.0910e+27],\n",
       "        [5.0740e+24],\n",
       "        [3.4727e+34],\n",
       "        [2.3017e+35],\n",
       "        [3.2336e+28],\n",
       "        [2.7049e+30],\n",
       "        [4.4314e+35],\n",
       "        [5.4146e+35],\n",
       "        [2.4738e+26],\n",
       "        [5.4623e-02],\n",
       "        [1.4214e+27],\n",
       "        [2.0834e+03],\n",
       "        [1.6934e+31],\n",
       "        [5.7199e+24],\n",
       "        [2.6743e+33],\n",
       "        [1.3613e+00],\n",
       "        [2.3991e+29],\n",
       "        [6.3166e-01],\n",
       "        [1.2940e+32],\n",
       "        [2.0341e+32],\n",
       "        [1.8355e+20],\n",
       "        [5.3216e+29],\n",
       "        [3.3139e+37],\n",
       "        [2.1289e+32],\n",
       "        [9.4134e+30],\n",
       "        [2.2959e+23],\n",
       "        [4.0732e+01],\n",
       "        [3.0231e+36],\n",
       "        [1.3527e+31],\n",
       "        [1.4886e+33],\n",
       "        [6.6021e+29],\n",
       "        [2.8668e+28],\n",
       "        [1.8638e+25],\n",
       "        [1.0055e+24],\n",
       "        [1.3030e+31],\n",
       "        [2.9949e+37],\n",
       "        [7.2119e+24],\n",
       "        [3.1412e+30],\n",
       "        [8.0242e+31],\n",
       "        [3.1431e+43],\n",
       "        [4.6721e+21],\n",
       "        [9.1090e+35],\n",
       "        [8.6756e+24],\n",
       "        [5.8229e+31],\n",
       "        [1.3860e+30],\n",
       "        [2.5114e+33],\n",
       "        [1.1259e+24],\n",
       "        [7.3586e-01],\n",
       "        [5.2905e+26],\n",
       "        [3.9134e+36],\n",
       "        [1.0925e+29],\n",
       "        [9.6755e+26],\n",
       "        [1.0082e+19],\n",
       "        [1.3923e+39],\n",
       "        [7.2543e+34],\n",
       "        [2.6295e+29],\n",
       "        [7.1227e+06],\n",
       "        [7.3180e+20],\n",
       "        [9.8805e+24],\n",
       "        [1.0181e+27],\n",
       "        [2.6423e+22],\n",
       "        [1.7376e+33],\n",
       "        [2.7563e+02],\n",
       "        [2.9866e+39],\n",
       "        [4.7171e+00],\n",
       "        [2.6849e+23],\n",
       "        [3.2194e+22],\n",
       "        [7.1078e+24],\n",
       "        [6.0094e+25],\n",
       "        [6.2673e-01],\n",
       "        [2.0027e+30],\n",
       "        [3.0921e+32],\n",
       "        [3.7682e+28],\n",
       "        [2.4434e+21],\n",
       "        [1.1559e+31],\n",
       "        [8.7482e+23],\n",
       "        [5.9468e+24],\n",
       "        [4.8550e+25],\n",
       "        [3.4413e+29],\n",
       "        [1.6538e+24],\n",
       "        [2.0168e+17],\n",
       "        [4.4299e+34],\n",
       "        [4.1098e+38],\n",
       "        [1.3407e+04],\n",
       "        [1.4194e+41],\n",
       "        [4.2536e+34],\n",
       "        [5.7156e+31],\n",
       "        [3.7009e+00],\n",
       "        [2.1713e+30],\n",
       "        [8.2555e+31],\n",
       "        [1.7548e+03],\n",
       "        [1.5533e+33],\n",
       "        [8.5574e+26],\n",
       "        [2.7031e+32],\n",
       "        [3.0007e+27],\n",
       "        [2.9036e+29],\n",
       "        [2.9775e+36],\n",
       "        [5.3728e+29],\n",
       "        [3.8525e+29],\n",
       "        [2.5940e+00],\n",
       "        [1.7665e+29],\n",
       "        [5.1670e+19],\n",
       "        [6.2667e+32],\n",
       "        [1.0147e+40],\n",
       "        [3.1781e+21],\n",
       "        [1.0682e+29],\n",
       "        [8.2347e+22],\n",
       "        [3.4889e+30],\n",
       "        [2.1193e+01],\n",
       "        [3.3701e+24],\n",
       "        [1.0369e+28],\n",
       "        [5.1587e+34],\n",
       "        [7.5096e+35],\n",
       "        [6.5601e+00],\n",
       "        [2.8196e+14],\n",
       "        [4.9224e+00],\n",
       "        [1.7370e+21],\n",
       "        [4.7059e+26],\n",
       "        [5.8819e+33],\n",
       "        [1.1613e+30],\n",
       "        [5.0353e+42],\n",
       "        [5.3253e+28],\n",
       "        [9.4562e+25],\n",
       "        [3.4772e+35],\n",
       "        [3.1092e-01],\n",
       "        [4.9032e+28],\n",
       "        [4.8856e+30],\n",
       "        [1.7345e+37],\n",
       "        [8.6354e+27],\n",
       "        [2.4887e+32],\n",
       "        [9.6560e+26],\n",
       "        [2.1621e+25],\n",
       "        [2.4486e+27],\n",
       "        [1.6637e+00],\n",
       "        [4.8449e+00],\n",
       "        [4.6484e+32],\n",
       "        [1.4485e+39],\n",
       "        [1.4754e+24],\n",
       "        [3.6141e+29],\n",
       "        [8.6432e+26],\n",
       "        [9.6679e+31],\n",
       "        [3.1325e+22],\n",
       "        [1.8813e+32],\n",
       "        [1.3721e+25],\n",
       "        [4.6513e+31],\n",
       "        [2.4928e+26],\n",
       "        [8.6050e+31],\n",
       "        [5.2467e+32],\n",
       "        [2.2210e+29],\n",
       "        [2.6063e+27],\n",
       "        [8.4702e+30],\n",
       "        [9.3585e+00],\n",
       "        [2.7950e+29],\n",
       "        [1.2388e+37],\n",
       "        [2.8800e+33],\n",
       "        [5.7483e+27],\n",
       "        [8.7286e+22],\n",
       "        [4.1806e+29],\n",
       "        [3.8077e+37],\n",
       "        [2.7423e+30],\n",
       "        [1.7801e+30],\n",
       "        [6.5402e+28],\n",
       "        [3.3656e+26],\n",
       "        [1.3879e+34],\n",
       "        [8.1445e+30],\n",
       "        [2.9259e+30],\n",
       "        [3.3847e+29],\n",
       "        [1.0227e+29],\n",
       "        [1.9685e+32],\n",
       "        [2.3802e+28],\n",
       "        [5.5020e+28],\n",
       "        [1.3553e+04],\n",
       "        [5.2761e+28],\n",
       "        [3.3663e+32],\n",
       "        [7.8978e+32],\n",
       "        [1.2729e+35],\n",
       "        [1.6432e+07],\n",
       "        [9.6256e+31],\n",
       "        [5.5629e+26],\n",
       "        [3.2075e+01],\n",
       "        [2.0301e+23],\n",
       "        [1.0291e+30],\n",
       "        [5.1084e+28],\n",
       "        [1.6517e+22],\n",
       "        [8.6305e+31],\n",
       "        [6.0794e+27],\n",
       "        [2.9693e+26],\n",
       "        [1.3306e+24],\n",
       "        [4.1024e+30],\n",
       "        [1.9910e+27],\n",
       "        [1.2345e+29],\n",
       "        [1.4029e+30],\n",
       "        [1.0806e+24],\n",
       "        [3.2863e+34],\n",
       "        [3.0823e+19],\n",
       "        [2.5520e-01],\n",
       "        [5.5229e+29],\n",
       "        [1.1420e+04],\n",
       "        [6.9653e+23],\n",
       "        [1.3560e+02],\n",
       "        [8.7648e+34],\n",
       "        [6.0079e+31],\n",
       "        [1.1762e+26],\n",
       "        [5.3410e+25],\n",
       "        [1.7018e+31],\n",
       "        [6.6676e+00],\n",
       "        [4.0760e+00],\n",
       "        [1.1782e+21],\n",
       "        [1.2248e+00],\n",
       "        [3.8319e+28],\n",
       "        [1.1995e+00],\n",
       "        [2.4072e+25],\n",
       "        [2.9997e+00],\n",
       "        [2.0348e+24],\n",
       "        [2.1818e+22],\n",
       "        [3.0243e+14],\n",
       "        [1.2426e+28],\n",
       "        [2.6466e+29],\n",
       "        [7.7217e+30],\n",
       "        [8.4652e+19],\n",
       "        [5.2630e+26],\n",
       "        [3.2197e+04],\n",
       "        [2.9361e+09],\n",
       "        [1.0025e+02],\n",
       "        [3.2389e+31],\n",
       "        [2.7908e+24],\n",
       "        [2.1504e+25],\n",
       "        [2.2124e+33],\n",
       "        [3.3333e+32],\n",
       "        [5.6864e+30],\n",
       "        [7.5653e+26],\n",
       "        [1.1620e+02],\n",
       "        [9.5085e+27],\n",
       "        [1.7565e+23],\n",
       "        [3.4771e+33],\n",
       "        [4.2182e+26],\n",
       "        [9.2357e-01],\n",
       "        [1.0292e+28],\n",
       "        [2.9386e+28],\n",
       "        [2.6006e+31],\n",
       "        [1.7100e+28],\n",
       "        [4.8048e-01],\n",
       "        [5.9525e+01],\n",
       "        [5.0670e+34],\n",
       "        [7.3669e+31],\n",
       "        [2.5346e+26],\n",
       "        [6.4470e-01],\n",
       "        [1.1397e+23],\n",
       "        [4.0998e+31],\n",
       "        [1.7996e+33],\n",
       "        [4.4186e+29],\n",
       "        [3.2139e+28],\n",
       "        [2.6641e+32],\n",
       "        [4.0907e+04],\n",
       "        [1.8723e+33],\n",
       "        [1.8778e+27],\n",
       "        [4.8118e+17],\n",
       "        [8.9682e+27],\n",
       "        [1.0353e+35],\n",
       "        [5.1725e+40],\n",
       "        [3.9431e+38],\n",
       "        [1.8832e+22],\n",
       "        [6.9848e+32],\n",
       "        [2.2861e+28],\n",
       "        [1.8112e+02],\n",
       "        [5.7915e+32],\n",
       "        [4.0448e+28],\n",
       "        [2.0948e+30],\n",
       "        [1.0307e+30],\n",
       "        [2.3281e+28],\n",
       "        [8.1478e+32],\n",
       "        [5.6636e+30],\n",
       "        [1.7650e+25],\n",
       "        [4.0875e+31],\n",
       "        [2.5339e+42],\n",
       "        [1.0435e+33],\n",
       "        [4.4023e+38],\n",
       "        [2.8924e+32],\n",
       "        [5.3473e+24],\n",
       "        [1.4938e+30],\n",
       "        [1.7504e+26],\n",
       "        [9.9525e+30],\n",
       "        [3.5044e+43],\n",
       "        [8.5626e+31],\n",
       "        [9.8527e+26],\n",
       "        [8.8044e+31],\n",
       "        [1.5318e+31],\n",
       "        [1.9689e+25],\n",
       "        [9.0489e+26],\n",
       "        [8.0460e+36],\n",
       "        [3.6570e+31],\n",
       "        [3.2039e+24],\n",
       "        [1.3453e+16],\n",
       "        [1.3095e+28],\n",
       "        [3.3394e+42],\n",
       "        [2.0503e+34],\n",
       "        [2.5924e+04],\n",
       "        [7.0328e+01],\n",
       "        [4.1411e+28],\n",
       "        [1.2370e+23],\n",
       "        [2.9153e+29],\n",
       "        [6.6115e+31],\n",
       "        [1.9846e+32],\n",
       "        [5.6985e+38],\n",
       "        [5.5671e+30],\n",
       "        [4.4955e+16],\n",
       "        [1.1859e+32],\n",
       "        [4.1693e+27],\n",
       "        [1.7716e+26],\n",
       "        [1.1689e+24],\n",
       "        [9.8329e-01],\n",
       "        [2.3748e+28],\n",
       "        [3.0283e+14],\n",
       "        [2.4283e+26],\n",
       "        [5.4174e+32],\n",
       "        [2.8460e+40],\n",
       "        [4.6456e+28],\n",
       "        [8.0898e+26],\n",
       "        [2.1834e+35],\n",
       "        [8.2993e+00],\n",
       "        [6.9718e+02],\n",
       "        [2.2241e+21],\n",
       "        [6.7676e+25],\n",
       "        [6.2958e+00],\n",
       "        [6.7350e+20],\n",
       "        [4.3066e+34],\n",
       "        [8.5920e+21],\n",
       "        [3.2913e+24],\n",
       "        [1.5728e+23],\n",
       "        [4.5586e+24],\n",
       "        [1.5794e+28]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(action_log_probs.double()-old_action_log_probs_batch.double())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varibad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
