{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('./algorithms/')\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, policy, encoder):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def get_actor_params(self):\n",
    "        return self.policy.get_actor_params()\n",
    "\n",
    "    def get_critic_params(self):\n",
    "        return self.policy.get_critic_params()\n",
    "\n",
    "    def forward_actor(self, inputs):\n",
    "        return self.policy.forward_actor(inputs)\n",
    "\n",
    "    def forward_critic(self, inputs):\n",
    "        return self.policy.forward_critic(inputs)\n",
    "    \n",
    "    def act(self, state, latent, belief=None, task=None, deterministic = False):\n",
    "        return self.policy.act(state, latent, belief, task, deterministic)\n",
    "\n",
    "    def get_value(self, state, latent, belief=None, task=None):\n",
    "        value, _ = self.policy.forward(state, latent, belief, task)\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, state, latent, belief, task, action):\n",
    "        \"\"\"Call policy eval, set task, belief to None\"\"\"\n",
    "        return self.policy.evaluate_actions(state, latent, belief, task, action)\n",
    "    \n",
    "        ## TODO: what to do about 'sample'? check what this arg is?\n",
    "    # def forward(self, actions, states, rewards, hidden_state, return_prior=False, sample=True, detach_every=None):\n",
    "    #     # really want this to take the inputs for the encoder and then output the outputs of the policy\n",
    "    #     # we only want to get the prior when there are no previous rewards, actions or hidden states\n",
    "    #     # should only occur at the very start of the continual learning process\n",
    "    #     if hidden_state is None:\n",
    "    #         # print('Hidden state is None!!:', hidden_state)\n",
    "    #         _, latent_mean, latent_logvar, hidden_state = self.encoder.prior(states.shape[1]) # check that this gets the batch size?\n",
    "    #     else:\n",
    "    #         _, latent_mean, latent_logvar, hidden_state = self.encoder(actions, states, rewards, hidden_state, return_prior, sample, detach_every)\n",
    "        \n",
    "    #     latent_mean = F.relu(latent_mean)\n",
    "    #     latent_logvar = F.relu(latent_logvar)\n",
    "    #     latent = torch.cat((latent_mean, latent_logvar), dim=-1).reshape(1, -1)\n",
    "    #     # none for belief and task\n",
    "    #     return self.policy(states, latent, None, None), hidden_state, latent\n",
    "    \n",
    "    # def prior(self, num_processes):\n",
    "    #     return self.encoder.prior(num_processes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get RL2 trained policy for example\n",
    "RUN_FOLDER = './logs/logs_ML10-v2/rl2_73__25:10_21:13:08'\n",
    "policy_net = torch.load(RUN_FOLDER + '/models/policy.pt')\n",
    "encoder_net = torch.load(RUN_FOLDER + '/models/encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. get prior at start for base latent\n",
    "# (does this reset the hidden state? I think so)\n",
    "#2. feed policy observation + latent -> gets action, obs, reward, done\n",
    "#3. feed encoder action, obs, reward, done and hidden state to get next action\n",
    "import metaworld\n",
    "import random\n",
    "\n",
    "ml10 = metaworld.ML10() # Construct the benchmark, sampling tasks\n",
    "\n",
    "training_envs = []\n",
    "for name, env_cls in ml10.test_classes.items():\n",
    "  env = env_cls()\n",
    "  task = random.choice([task for task in ml10.test_tasks\n",
    "                        if task.env_name == name])\n",
    "  env.set_task(task)\n",
    "  training_envs.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.env_utils.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from environments.parallel_envs import VecPyTorch\n",
    "\n",
    "def make_continual_env(env_id, **kwargs):\n",
    "    def _thunk():\n",
    "\n",
    "        env = gym.make(env_id, **kwargs)\n",
    "        # if tasks is not None:\n",
    "        #     env.unwrapped.reset_task = lambda x: env.unwrapped.set_task(random.choice(tasks))\n",
    "        # if seed is not None:\n",
    "        #     env.seed(seed + rank)\n",
    "        # if str(env.__class__.__name__).find('TimeLimit') >= 0:\n",
    "        #     env = TimeLimitMask(env)\n",
    "        # env = VariBadWrapper(env=env, episodes_per_task=episodes_per_task, add_done_info=add_done_info)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "vec_envs = SubprocVecEnv([make_continual_env('continualMW-v0', **{'envs' : training_envs}) for i in range(4)])\n",
    "# pyt_vec = VecPyTorch(vec_envs, device)\n",
    "# pyt_vec2 = VecPyTorch([make_continual_env('continualMW-v0', **{'envs' : training_envs}) for i in range(4)], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from algorithms.custom_storage import CustomOnlineStorage\n",
    "from algorithms.custom_ppo import CustomPPO\n",
    "from environments.metaworld_envs.test_continual_env import ContinualEnv\n",
    "\n",
    "num_processes = 4\n",
    "# combined network with encoder + policy\n",
    "ac = ActorCritic(policy_net, encoder_net)\n",
    "agent = CustomPPO(\n",
    "    actor_critic=ac,\n",
    "    value_loss_coef = 1,\n",
    "    entropy_coef = 0.001,\n",
    "    policy_optimiser='adam',\n",
    "    policy_anneal_lr=False,\n",
    "    train_steps = 2,\n",
    "    lr = 0.001,\n",
    "    eps=1.0e-8,\n",
    "    clip_param = 0.2,\n",
    "    ppo_epoch = 3,\n",
    "    use_huber_loss = True,\n",
    "    use_clipped_value_loss=True,\n",
    "    context_window=None\n",
    ")\n",
    "# env = training_envs[0]\n",
    "# env = ContinualEnv(training_envs, 500)\n",
    "envs = SubprocVecEnv([make_continual_env('continualMW-v0', **{'envs' : training_envs}) for i in range(num_processes)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: make sure this can get all other properties of envs\n",
    "from environments.env_utils.vec_env import VecEnvWrapper\n",
    "class PyTorchVecEnvCont(VecEnvWrapper):\n",
    "\n",
    "    def __init__(self, vec_envs, device):\n",
    "        super(PyTorchVecEnvCont, self).__init__(vec_envs)\n",
    "        # self.vec_envs = vec_envs\n",
    "        self.device = device\n",
    "\n",
    "    # def step_wait(self, actions)\n",
    "\n",
    "    # def step(self, actions):\n",
    "    #     actions.cpu().detach()\n",
    "    #     obs, reward, done, info = self.vec_envs.step(actions)\n",
    "    #     obs = torch.from_numpy(obs).permute(1, 0, 2).to(self.device)\n",
    "    #     reward = torch.from_numpy(reward).to(self.device)\n",
    "    #     return obs, reward, done, info\n",
    "    \n",
    "    def step_async(self, actions):\n",
    "        # actions = actions.squeeze(1).cpu().numpy()\n",
    "        # convert actions for worker .permute(1, 0, 2)\n",
    "        actions = actions.permute(1, 0, 2).squeeze().cpu().numpy()\n",
    "        self.venv.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        state, reward, done, info = self.venv.step_wait()\n",
    "        if isinstance(state, list):  # raw + normalised .permute(1, 0, 2)\n",
    "            state = [torch.from_numpy(s).float().to(self.device) for s in state]\n",
    "        else:\n",
    "            state = torch.from_numpy(state).permute(1, 0, 2).float().to(self.device)\n",
    "        # reshape rewards to have dim T X B X D .reshape(1, -1, 1)\n",
    "        if isinstance(reward, list):  # raw + normalised\n",
    "            reward = [torch.from_numpy(r).unsqueeze(dim=1).reshape(1, -1, 1).float().to(self.device) for r in reward]\n",
    "        else:\n",
    "            reward = torch.from_numpy(reward).unsqueeze(dim=1).reshape(1, -1, 1).float().to(self.device)\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # if task is not None:\n",
    "        #     assert isinstance(task, list)\n",
    "        state = self.venv.reset()\n",
    "        ## permute state to have dimensions T X B X D .permute(1,0,2)\n",
    "        if isinstance(state, list):\n",
    "            state = [torch.from_numpy(s).float().to(self.device) for s in state]\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().to(self.device)\n",
    "        return state\n",
    "    \n",
    "    # def reset(self):\n",
    "    #     obs = self.vec_envs.reset()\n",
    "    #     obs = torch.from_numpy(obs).permute(1, 0, 2).to(self.device)\n",
    "    #     return obs\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\" If env does not have the attribute then call the attribute in the wrapped_env \"\"\"\n",
    "\n",
    "        if attr in ['_max_episode_steps', 'task_dim', 'belief_dim', 'num_states']:\n",
    "            return self.unwrapped.get_env_attr(attr)\n",
    "\n",
    "        try:\n",
    "            orig_attr = self.__getattribute__(attr)\n",
    "        except AttributeError:\n",
    "            orig_attr = self.unwrapped.__getattribute__(attr)\n",
    "\n",
    "        if callable(orig_attr):\n",
    "            def hooked(*args, **kwargs):\n",
    "                result = orig_attr(*args, **kwargs)\n",
    "                return result\n",
    "\n",
    "            return hooked\n",
    "        else:\n",
    "            return orig_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step: 0; limit: 50000\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:84: operator(): block: [21,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/grant/working_repos/varibad/continual_training_test.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39m### update\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39m# if step % num_updates ==0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# update at the end of each episode?\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m agent\u001b[39m.\u001b[39;49mupdate(storage)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# should clear out old data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m storage\u001b[39m.\u001b[39mafter_update()\n",
      "File \u001b[0;32m~/working_repos/varibad/algorithms/custom_ppo.py:93\u001b[0m, in \u001b[0;36mCustomPPO.update\u001b[0;34m(self, policy_storage, encoder, rlloss_through_encoder, compute_vae_loss)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mppo_epoch):\n\u001b[1;32m     92\u001b[0m     data_generator \u001b[39m=\u001b[39m policy_storage\u001b[39m.\u001b[39mfeed_forward_generator(advantages, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_mini_batch)\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data_generator:\n\u001b[1;32m     95\u001b[0m         state_batch, actions_batch, latent_batch, value_preds_batch, \\\n\u001b[1;32m     96\u001b[0m         return_batch, old_action_log_probs_batch, adv_targ \u001b[39m=\u001b[39m sample\n\u001b[1;32m     98\u001b[0m         \u001b[39m# if not rlloss_through_encoder:\u001b[39;00m\n",
      "File \u001b[0;32m~/working_repos/varibad/algorithms/custom_storage.py:287\u001b[0m, in \u001b[0;36mCustomOnlineStorage.feed_forward_generator\u001b[0;34m(self, advantages, num_mini_batch, mini_batch_size)\u001b[0m\n\u001b[1;32m    267\u001b[0m latent_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatent)[indices]\n\u001b[1;32m    268\u001b[0m \u001b[39m# if self.args.pass_state_to_policy:\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m#     state_batch = self.prev_state[:-1].reshape(-1, *self.prev_state.size()[2:])[indices]\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39m#     task_batch = None\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m actions_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))[indices]\n\u001b[1;32m    289\u001b[0m value_preds_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_preds[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)[indices]\n\u001b[1;32m    290\u001b[0m return_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturns[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)[indices]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "# num_episodes_per_update = 4\n",
    "envs = SubprocVecEnv([make_continual_env('continualMW-v0', **{'envs' : training_envs}) for i in range(num_processes)])\n",
    "env = PyTorchVecEnvCont(envs, device)\n",
    "storage = CustomOnlineStorage(\n",
    "    500, num_processes, env.observation_space.shape[0]+1, 0, 0,\n",
    "    env.action_space, ac.encoder.hidden_size, ac.encoder.latent_dim, False)\n",
    "\n",
    "while env.get_env_attr('cur_step') < env.get_env_attr('steps_limit'):\n",
    "    print(f\"current step: {env.get_env_attr('cur_step')}; limit: {env.get_env_attr('steps_limit')}\")\n",
    "    step = 0\n",
    "    \n",
    "    # if I do this, I need to make sure my returns are calculated correctly / the done flags work\n",
    "    # for i in range(num_episodes_per_update):\n",
    "    obs = env.reset()\n",
    "    done = [False for _ in range(num_processes)]\n",
    "    # print(f\"running episode {i}\")\n",
    "    ## get prior??? how frequent?\n",
    "    # do at start of each episode for now\n",
    "    with torch.no_grad():\n",
    "        _, latent_mean, latent_logvar, hidden_state = agent.actor_critic.encoder.prior(num_processes)\n",
    "        print(step)\n",
    "        ## TODO: set the 500 to some sort of variable (max episode len?)\n",
    "        assert len(storage.latent) == 0  # make sure we emptied buffers\n",
    "        # print(f\"saving hidden state to {i * 500}\")\n",
    "        storage.hidden_states[:1].copy_(hidden_state)\n",
    "        latent = torch.cat((latent_mean.clone(), latent_logvar.clone()), dim=-1)#.reshape(1, -1)\n",
    "        storage.latent.append(latent)\n",
    "\n",
    "    while not all(done):\n",
    "        value, action = agent.act(obs, latent, None, None)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        assert all(done) == any(done), \"Metaworld envs should all end simultaneously\"\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        ## TODO: do I even need masks?\n",
    "        # create mask for episode ends\n",
    "        masks_done = torch.FloatTensor([[0.0] if _done else [1.0] for _done in done]).to(device)\n",
    "        # bad_mask is true if episode ended because time limit was reached\n",
    "        # don't care for metaworld\n",
    "        bad_masks = torch.FloatTensor([[0.0] for _done in done]).to(device)\n",
    "\n",
    "        # if done:\n",
    "        #     print(f'{step}: done!')\n",
    "        #     hidden_state = agent.actor_critic.encoder.reset_hidden(hidden_state, masks_done)\n",
    "        # print(action.size(), obs.squeeze(0).size(), reward.squeeze(0).size(), hidden_state.size(), latent.size(), masks_done.size(), bad_masks.size())\n",
    "        _, latent_mean, latent_logvar, hidden_state = agent.actor_critic.encoder(action, obs, reward, hidden_state, return_prior = False)\n",
    "        latent = torch.cat((latent_mean.clone(), latent_logvar.clone()), dim = -1)[None,:]#.reshape(1, -1)\n",
    "\n",
    "        \n",
    "        storage.next_state[step] = obs.clone()\n",
    "        # print(action.squeeze(0).size(), obs.squeeze(0).size(), reward.squeeze(0).size(), hidden_state.size(), latent.size(), masks_done.size(), bad_masks.size())\n",
    "        storage.insert(\n",
    "            state=obs.squeeze(),\n",
    "            belief=None,\n",
    "            task=None,\n",
    "            actions=action,\n",
    "            rewards_raw=reward.squeeze(0),\n",
    "            rewards_normalised=reward.squeeze(0),#rew_normalised,\n",
    "            value_preds=value.squeeze(0),\n",
    "            masks=masks_done.squeeze(0), # do I even need these?\n",
    "            bad_masks=bad_masks.squeeze(0), \n",
    "            done=torch.from_numpy(done)[:,None].float(),\n",
    "            hidden_states = hidden_state.squeeze(),\n",
    "            latent = latent#.unsqueeze(1),\n",
    "        )\n",
    "\n",
    "        step += 1\n",
    "        ### update\n",
    "        # if step % num_updates ==0:\n",
    "\n",
    "    # update at the end of each episode?\n",
    "    agent.update(storage)\n",
    "    # should clear out old data\n",
    "    storage.after_update()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0037,  0.0568,  0.0397,  ..., -0.0980, -0.0445, -0.0333],\n",
       "         [ 0.0037,  0.0568,  0.0397,  ..., -0.0980, -0.0445, -0.0333],\n",
       "         [ 0.0037,  0.0568,  0.0397,  ..., -0.0980, -0.0445, -0.0333],\n",
       "         [ 0.0037,  0.0568,  0.0397,  ..., -0.0980, -0.0445, -0.0333]],\n",
       "\n",
       "        [[-0.0807,  0.1167,  0.0092,  ..., -0.1912, -0.2447,  0.0459],\n",
       "         [-0.0998,  0.1476,  0.0306,  ..., -0.1913, -0.2540,  0.0545],\n",
       "         [-0.0718,  0.0751, -0.0131,  ..., -0.2039, -0.2138,  0.0240],\n",
       "         [-0.1134,  0.1231,  0.0050,  ..., -0.2011, -0.2479,  0.0196]],\n",
       "\n",
       "        [[-0.1516,  0.1161, -0.0543,  ..., -0.2709, -0.3519,  0.0283],\n",
       "         [-0.2124,  0.1815, -0.0032,  ..., -0.2696, -0.3644,  0.0336],\n",
       "         [-0.0949,  0.0863, -0.0512,  ..., -0.2828, -0.3137,  0.0642],\n",
       "         [-0.1658,  0.1080, -0.0734,  ..., -0.2641, -0.3739,  0.0025]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6353, -0.4397, -0.9656,  ..., -0.5909, -0.4097, -0.1709],\n",
       "         [ 0.6032, -0.4428, -0.9768,  ..., -0.5935, -0.4159, -0.1913],\n",
       "         [ 0.5799, -0.4575, -0.9654,  ..., -0.5902, -0.4101, -0.2067],\n",
       "         [ 0.5499, -0.4012, -0.9929,  ..., -0.5696, -0.4774, -0.2066]],\n",
       "\n",
       "        [[ 0.6327, -0.4623, -0.9453,  ..., -0.5951, -0.3583, -0.1709],\n",
       "         [ 0.5665, -0.4398, -0.9926,  ..., -0.5834, -0.4365, -0.2161],\n",
       "         [ 0.6067, -0.4566, -0.9519,  ..., -0.5882, -0.3800, -0.1861],\n",
       "         [ 0.5664, -0.4223, -0.9520,  ..., -0.5784, -0.4249, -0.1890]],\n",
       "\n",
       "        [[ 0.5774, -0.4139, -0.9808,  ..., -0.5541, -0.4228, -0.2083],\n",
       "         [ 0.5692, -0.4235, -0.9743,  ..., -0.5664, -0.4305, -0.2146],\n",
       "         [ 0.5372, -0.4004, -0.9925,  ..., -0.5426, -0.4647, -0.2321],\n",
       "         [ 0.5480, -0.4206, -0.9421,  ..., -0.5636, -0.4157, -0.1994]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(storage.latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = [storage.latent[0].detach().clone()]\n",
    "latent[0].requires_grad = True\n",
    "\n",
    "h = storage.hidden_states[0].detach()\n",
    "for i in range(storage.actions.shape[0]):\n",
    "    # reset hidden state of the GRU when we reset the task\n",
    "    h = agent.actor_critic.encoder.reset_hidden(h, storage.done[i])\n",
    "    # not sure why this is i + 1?\n",
    "    # h = self.actor_critic.encoder.reset_hidden(h, policy_storage.done[i + 1])\n",
    "\n",
    "    _, tm, tl, h = agent.actor_critic.encoder(\n",
    "        storage.actions.float()[i:i + 1],\n",
    "        storage.next_state[i:i + 1],\n",
    "        storage.rewards_raw[i:i + 1],\n",
    "        h,\n",
    "        sample=False,\n",
    "        return_prior=False,\n",
    "        detach_every=None\n",
    "    )\n",
    "    # latent_sample.append(ts)\n",
    "    latent.append(torch.cat((tm, tl), dim = -1)[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cat(storage.latent) - torch.cat(latent)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.latent[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 256])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "storage.latent[i] - latent[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0807,  0.1167,  0.0092,  ..., -0.1912, -0.2447,  0.0459],\n",
       "        [-0.0998,  0.1476,  0.0306,  ..., -0.1913, -0.2540,  0.0545],\n",
       "        [-0.0718,  0.0751, -0.0131,  ..., -0.2039, -0.2138,  0.0240],\n",
       "        [-0.1134,  0.1231,  0.0050,  ..., -0.2011, -0.2479,  0.0196]],\n",
       "       device='cuda:0', grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2004, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(latent).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward.squeeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000]],\n",
       "\n",
       "        [[1.2308],\n",
       "         [1.2294],\n",
       "         [1.2258],\n",
       "         [1.2257]],\n",
       "\n",
       "        [[0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000]],\n",
       "\n",
       "        [[0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000]],\n",
       "\n",
       "        [[0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.hidden_states[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/grant/working_repos/varibad/environments/env_utils/vec_env/subproc_vec_env.py\", line 21, in worker\n",
      "    ob = env.reset()\n",
      "  File \"/home/grant/working_repos/varibad/environments/metaworld_envs/test_continual_env.py\", line 58, in reset\n",
      "    obs = torch.from_numpy(np.append(obs, 0).reshape(1, -1)).float().to(device)\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 163, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "Process Process-10:\n",
      "Process Process-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/grant/working_repos/varibad/environments/env_utils/vec_env/subproc_vec_env.py\", line 21, in worker\n",
      "    ob = env.reset()\n",
      "  File \"/home/grant/working_repos/varibad/environments/env_utils/vec_env/subproc_vec_env.py\", line 21, in worker\n",
      "    ob = env.reset()\n",
      "  File \"/home/grant/working_repos/varibad/environments/metaworld_envs/test_continual_env.py\", line 58, in reset\n",
      "    obs = torch.from_numpy(np.append(obs, 0).reshape(1, -1)).float().to(device)\n",
      "  File \"/home/grant/working_repos/varibad/environments/metaworld_envs/test_continual_env.py\", line 58, in reset\n",
      "    obs = torch.from_numpy(np.append(obs, 0).reshape(1, -1)).float().to(device)\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 163, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 163, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "Process Process-12:\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/grant/working_repos/varibad/environments/env_utils/vec_env/subproc_vec_env.py\", line 21, in worker\n",
      "    ob = env.reset()\n",
      "  File \"/home/grant/working_repos/varibad/environments/metaworld_envs/test_continual_env.py\", line 58, in reset\n",
      "    obs = torch.from_numpy(np.append(obs, 0).reshape(1, -1)).float().to(device)\n",
      "  File \"/home/grant/miniconda3/envs/varibad/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 163, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/grant/working_repos/varibad/continual_training_test.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m vec_envs\u001b[39m.\u001b[39;49mreset()\n",
      "File \u001b[0;32m~/working_repos/varibad/environments/env_utils/vec_env/subproc_vec_env.py:98\u001b[0m, in \u001b[0;36mSubprocVecEnv.reset\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes:\n\u001b[1;32m     97\u001b[0m     remote\u001b[39m.\u001b[39msend((\u001b[39m'\u001b[39m\u001b[39mreset\u001b[39m\u001b[39m'\u001b[39m, task))\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mstack([remote\u001b[39m.\u001b[39mrecv() \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes])\n",
      "File \u001b[0;32m~/working_repos/varibad/environments/env_utils/vec_env/subproc_vec_env.py:98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes:\n\u001b[1;32m     97\u001b[0m     remote\u001b[39m.\u001b[39msend((\u001b[39m'\u001b[39m\u001b[39mreset\u001b[39m\u001b[39m'\u001b[39m, task))\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mstack([remote\u001b[39m.\u001b[39;49mrecv() \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes])\n",
      "File \u001b[0;32m~/miniconda3/envs/varibad/lib/python3.9/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[1;32m    251\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39mloads(buf\u001b[39m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/miniconda3/envs/varibad/lib/python3.9/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    415\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/varibad/lib/python3.9/multiprocessing/connection.py:383\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    382\u001b[0m     \u001b[39mif\u001b[39;00m remaining \u001b[39m==\u001b[39m size:\n\u001b[0;32m--> 383\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgot end of file during message\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vec_envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reset() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/grant/working_repos/varibad/continual_training_test.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbox.x.agi.io/home/grant/working_repos/varibad/continual_training_test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m obs \u001b[39m=\u001b[39m pyt_vec\u001b[39m.\u001b[39;49mreset()\n",
      "File \u001b[0;32m~/working_repos/varibad/environments/parallel_envs.py:81\u001b[0m, in \u001b[0;36mVecPyTorch.reset\u001b[0;34m(self, index, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m task \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(task, \u001b[39mlist\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mreset(index\u001b[39m=\u001b[39;49mindex, task\u001b[39m=\u001b[39;49mtask)\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(state, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     83\u001b[0m     state \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mfrom_numpy(s)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m state]\n",
      "\u001b[0;31mTypeError\u001b[0m: reset() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "obs = pyt_vec.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Resetting tensor([[0.]], device='cuda:0') tensor([[ 0.0037,  0.0568,  0.0397, -0.0055, -0.0818, -0.0101,  0.0708,  0.0079,\n",
      "          0.0424,  0.1191, -0.0008, -0.0250, -0.0121, -0.0623, -0.0995, -0.0121,\n",
      "         -0.0115, -0.0719,  0.0463, -0.1275,  0.0058,  0.0608,  0.0959, -0.0609,\n",
      "          0.0416,  0.0727, -0.0562,  0.0961, -0.0383,  0.0316, -0.0229,  0.0256,\n",
      "         -0.0508,  0.0397,  0.0675, -0.0524,  0.0099, -0.0123,  0.0097,  0.0050,\n",
      "          0.0064, -0.0643,  0.0515, -0.0102,  0.0280,  0.0607,  0.0094, -0.0862,\n",
      "          0.0151, -0.0556,  0.0345,  0.0694, -0.0842, -0.0672, -0.0334, -0.0391,\n",
      "          0.0942,  0.0439,  0.0679,  0.0088, -0.0418,  0.0701, -0.0439,  0.0398,\n",
      "         -0.0556, -0.0207,  0.0492,  0.0810,  0.0516, -0.0498,  0.0138, -0.0205,\n",
      "          0.0046,  0.0644, -0.0265,  0.0170,  0.0024,  0.0657,  0.0330,  0.0506,\n",
      "          0.0059,  0.0069, -0.0340,  0.0629, -0.0139,  0.0307,  0.0692, -0.0419,\n",
      "         -0.0929,  0.0006, -0.0722,  0.0374,  0.0375,  0.0830,  0.0214,  0.0302,\n",
      "          0.0922,  0.0071,  0.0752,  0.0806,  0.0896, -0.0598,  0.0144, -0.0461,\n",
      "         -0.0877, -0.0639, -0.0396, -0.0764, -0.0227, -0.0292,  0.0437, -0.0548,\n",
      "          0.0222, -0.0037,  0.0667,  0.0547, -0.0451, -0.0908, -0.0285,  0.0040,\n",
      "          0.0156, -0.0248, -0.0859,  0.0630, -0.0858, -0.0698,  0.0794,  0.0273,\n",
      "          0.0305, -0.0227, -0.0715,  0.0116, -0.0491, -0.0133, -0.0831, -0.0449,\n",
      "          0.0625, -0.0768, -0.0846,  0.1129, -0.0603,  0.0229,  0.0877,  0.0335,\n",
      "          0.0953, -0.0228, -0.0453, -0.0632, -0.0164,  0.0099,  0.0427,  0.0614,\n",
      "         -0.0531, -0.0332,  0.0444, -0.0334, -0.0643, -0.0809, -0.1277,  0.0243,\n",
      "         -0.0984, -0.1039, -0.0209, -0.0463, -0.0227,  0.0971, -0.0807, -0.0238,\n",
      "          0.0556,  0.0668, -0.0541, -0.0211,  0.0504, -0.0455, -0.0365, -0.1034,\n",
      "          0.0160,  0.0569, -0.0736,  0.0997,  0.0891,  0.0686, -0.0919, -0.0338,\n",
      "         -0.1024,  0.0159, -0.1194,  0.0872,  0.0169, -0.0126,  0.0335, -0.0491,\n",
      "         -0.0349,  0.0667, -0.0827, -0.0335,  0.0291,  0.0007, -0.0762,  0.0637,\n",
      "         -0.0276, -0.0282, -0.0202, -0.0288, -0.0068, -0.0415, -0.0269, -0.0394,\n",
      "          0.0101,  0.0609, -0.0169, -0.0374,  0.0288,  0.0674,  0.0763, -0.1209,\n",
      "         -0.0535, -0.0344, -0.0450,  0.0096,  0.0618, -0.1026,  0.0410, -0.0932,\n",
      "         -0.0648,  0.0349,  0.0480, -0.0639, -0.0806, -0.0043,  0.0836,  0.0616,\n",
      "         -0.0289,  0.0359, -0.0260,  0.0019, -0.0915, -0.0279,  0.0507, -0.0948,\n",
      "         -0.0349, -0.0786,  0.0641,  0.0269,  0.0499, -0.0262,  0.0078,  0.0408,\n",
      "         -0.0183,  0.0802,  0.0136,  0.0843, -0.0154, -0.0980, -0.0445, -0.0333]],\n",
      "       device='cuda:0')\n",
      "1000 Resetting tensor([[0.]], device='cuda:0') tensor([[ 4.3979e-01, -2.3399e-01, -5.7902e-01,  2.4264e-01, -1.9479e-01,\n",
      "          6.6326e-01,  1.0876e-01,  1.7590e-01,  1.3492e+00, -7.4028e-01,\n",
      "          9.3311e-01, -8.0168e-02, -1.3670e-01, -1.3672e-01, -1.8110e-01,\n",
      "         -4.3790e-01, -6.5070e-01, -1.5693e-01,  1.3017e-01, -5.3195e-01,\n",
      "         -3.8031e-01, -5.9328e-01, -6.8761e-01, -2.7469e-01,  3.7795e-01,\n",
      "         -4.4068e-01, -7.4714e-02, -4.9112e-01, -5.0766e-02, -4.5623e-01,\n",
      "         -8.8921e-02,  5.6067e-01, -3.2057e-01,  4.1361e-01,  8.4787e-01,\n",
      "          4.2339e-01, -3.6598e-01, -1.3970e-01,  1.1415e+00,  1.1299e+00,\n",
      "          1.5597e-01,  9.2714e-01,  4.1813e-03, -3.0106e-01, -1.9313e-01,\n",
      "         -5.6784e-02,  7.0606e-01, -4.4209e-01, -4.4526e-01, -4.7536e-01,\n",
      "         -3.1383e-01,  1.2964e-01, -1.9119e-01,  1.0127e+00, -3.0055e-01,\n",
      "         -1.4529e-01, -9.3701e-02,  9.7704e-03, -9.1542e-01,  1.6693e+00,\n",
      "          6.9395e-01, -8.2286e-01,  8.8857e-03,  3.6316e-01, -1.7860e-01,\n",
      "         -4.6169e-01, -9.4342e-02, -3.3169e-03, -2.4281e-01, -3.5091e-01,\n",
      "          1.1574e+00,  2.1035e-01,  3.2144e-01, -4.0178e-01,  1.0050e-01,\n",
      "          5.6252e-01, -6.1883e-01, -3.9167e-01, -4.3186e-01,  6.7908e-01,\n",
      "         -3.5475e-01,  4.6116e-01, -7.8788e-02,  3.3648e-01, -2.4526e-01,\n",
      "         -6.0998e-01, -6.1500e-01, -2.2815e-01, -3.2558e-01, -1.0717e-01,\n",
      "         -7.6436e-02, -4.7398e-01,  3.6296e-01,  1.2471e-01,  6.2921e-01,\n",
      "         -1.4147e-01, -4.8147e-01, -4.5904e-01, -2.6337e-01, -9.3517e-01,\n",
      "         -2.6050e-01,  1.0983e+00,  1.0813e+00,  6.9507e-01,  9.5539e-01,\n",
      "          5.5018e-01,  3.6905e-01, -2.5452e-01, -5.2612e-01, -5.3200e-01,\n",
      "          5.0627e-01, -1.4445e-01, -2.5475e-02,  2.1204e-01, -6.9729e-02,\n",
      "         -8.1060e-01, -5.5303e-02, -3.9797e-01, -3.1564e-01,  2.2305e-01,\n",
      "         -2.0754e-01,  3.5696e-01, -1.5590e-01,  2.3546e-01, -2.9399e-01,\n",
      "         -2.8245e-01,  1.4093e+00,  8.5420e-02, -1.2425e-01, -1.2681e-01,\n",
      "         -2.0919e-01, -3.3708e-01,  1.6138e-02, -5.5742e-01,  1.4796e+00,\n",
      "          3.3051e-01, -5.7010e-01, -3.8022e-01, -1.5282e-01, -1.7140e-01,\n",
      "         -2.0521e-01, -4.8020e-01, -5.2734e-02, -2.2959e-01,  5.7475e-01,\n",
      "          1.6532e-01,  3.0663e-01, -4.1724e-01,  2.0389e-01,  2.9942e-01,\n",
      "         -1.6048e-01, -4.2324e-01, -1.4167e-01, -3.2306e-01, -7.0928e-01,\n",
      "          3.5496e-01, -2.6563e-01,  1.0621e+00, -1.4753e+00,  2.6048e-01,\n",
      "         -2.7083e-01,  1.3841e-02,  3.4142e-01,  5.9346e-02, -3.4134e-01,\n",
      "          1.0621e+00,  3.7375e-01, -9.9363e-02,  1.6973e+00, -2.3250e-01,\n",
      "          3.8408e-01, -3.8401e-01, -3.7486e-01, -4.9533e-01, -3.0926e-01,\n",
      "          9.7299e-02, -4.5667e-01,  5.4441e-01, -8.6375e-02, -1.6075e-01,\n",
      "         -9.4462e-01, -2.4635e-01, -1.5484e-01,  1.2983e+00, -3.0645e-01,\n",
      "         -1.4800e-01, -2.0214e-01, -5.1349e-01, -2.8827e-01, -3.4795e-01,\n",
      "         -1.6620e-01, -2.3735e-01, -4.2800e-01, -5.3985e-01,  1.5129e-01,\n",
      "         -2.7171e-02,  6.8775e-01,  6.0589e-01,  1.5184e+00, -7.7306e-01,\n",
      "         -2.6281e-01, -3.2857e-01,  9.3584e-02,  8.8754e-01,  7.0602e-01,\n",
      "          8.6251e-01, -2.3734e-01,  1.1007e+00, -5.1956e-01, -6.7776e-01,\n",
      "         -2.2194e-01, -1.5324e-01,  2.3211e-01,  2.6340e-01, -2.4425e-01,\n",
      "         -4.9674e-01, -2.3426e-01, -1.5213e-01,  3.4948e-01, -4.2482e-02,\n",
      "         -6.2706e-02, -1.1174e-01, -4.0601e-01, -2.9196e-01, -2.2637e-01,\n",
      "          2.2549e-03,  3.7716e-01, -2.7323e-01,  6.1158e-01, -7.6139e-02,\n",
      "         -2.8563e-01,  3.9521e-01, -7.0878e-01, -1.4894e-01, -8.9535e-02,\n",
      "         -3.4153e-01, -3.7062e-01, -1.5903e-01, -1.2897e-01,  5.1950e-02,\n",
      "         -1.3720e-03, -2.2604e-01, -2.3796e-01, -1.0929e-01,  4.6072e-01,\n",
      "         -1.5313e-01, -5.0545e-01,  1.2610e+00,  5.8361e-01, -5.4962e-01,\n",
      "          5.6784e-01,  4.2190e-01, -4.3476e-01, -3.8871e-01, -2.4028e-01,\n",
      "         -1.2053e-01]], device='cuda:0')\n",
      "1500 Resetting tensor([[0.]], device='cuda:0') tensor([[ 7.2061e-01, -4.3451e-01, -9.6629e-01,  3.2825e-01, -1.3288e-01,\n",
      "          9.2859e-01, -7.0005e-02,  3.0722e-02,  1.9471e+00, -8.2754e-01,\n",
      "          1.2613e+00, -1.3220e-03, -2.6858e-02, -9.6500e-02, -9.2540e-02,\n",
      "         -5.2164e-01, -7.7209e-01, -6.7330e-02, -5.7737e-02, -5.5618e-01,\n",
      "         -4.6691e-01, -1.2050e+00, -9.6543e-01, -2.3156e-01,  3.1366e-01,\n",
      "         -7.2911e-01, -5.7634e-01, -5.8841e-01, -5.7423e-01, -6.8566e-01,\n",
      "         -4.9237e-01,  1.0757e+00, -3.1761e-01,  4.5271e-01,  1.1915e+00,\n",
      "          4.7777e-01, -7.5824e-01, -6.5457e-02,  1.7093e+00,  1.6684e+00,\n",
      "          2.4338e-01,  1.4931e+00,  1.4315e-02, -4.7872e-01, -7.1932e-01,\n",
      "         -1.1714e-01,  1.0074e+00, -7.7551e-01, -3.8899e-01, -3.7250e-01,\n",
      "         -3.3802e-01,  2.8607e-01, -1.2236e-01,  1.4429e+00, -4.8693e-02,\n",
      "         -7.1722e-02, -1.6063e-01,  7.1998e-02, -1.0579e+00,  2.4631e+00,\n",
      "          1.0963e+00, -1.3320e+00, -1.0974e-02,  3.1653e-01, -3.1283e-01,\n",
      "         -4.6620e-01, -3.4104e-01,  2.6044e-01, -1.8076e-01, -2.6473e-01,\n",
      "          1.7650e+00,  2.2705e-01,  1.8427e-01, -5.9022e-01, -3.7367e-01,\n",
      "          7.1888e-01, -1.0088e+00, -6.0247e-01, -4.0632e-01,  1.1792e+00,\n",
      "         -7.8704e-01,  7.7312e-01, -4.0022e-01,  2.7616e-01, -3.5216e-01,\n",
      "         -5.9003e-01, -1.0552e+00, -2.1659e-01, -4.3130e-01, -1.3851e-01,\n",
      "         -6.8128e-02, -9.5526e-01,  4.4805e-01, -3.3443e-01,  6.2447e-01,\n",
      "         -3.6657e-01, -6.2679e-01, -7.7295e-01, -3.8836e-01, -1.4006e+00,\n",
      "         -5.5793e-01,  1.6555e+00,  1.4823e+00,  1.0824e+00,  1.3758e+00,\n",
      "          8.0706e-01,  5.8839e-01, -2.8714e-01, -8.2476e-01, -6.7192e-01,\n",
      "          8.2809e-01, -9.3567e-02,  2.4358e-02,  4.6811e-02, -1.9337e-01,\n",
      "         -1.1382e+00, -9.0944e-02, -4.7257e-01, -4.0617e-01,  1.2202e-02,\n",
      "         -2.1092e-01,  6.5066e-01, -1.7343e-01,  8.2796e-02, -1.2922e-01,\n",
      "         -6.7495e-02,  1.9770e+00, -1.5912e-01, -1.5655e-01, -9.3999e-02,\n",
      "         -6.5431e-02, -4.8011e-01, -3.0353e-01, -6.3767e-01,  1.8873e+00,\n",
      "          5.7210e-01, -7.0731e-01, -4.9757e-01, -1.2821e-01, -4.3196e-01,\n",
      "         -8.9399e-02, -4.2889e-01, -2.2507e-02, -1.7887e-01,  1.1473e+00,\n",
      "         -7.3143e-02, -1.9728e-01, -5.1249e-01,  4.1299e-01,  4.6314e-01,\n",
      "         -3.4243e-01, -5.4569e-01, -4.4101e-02, -2.1667e-01, -8.8832e-01,\n",
      "          3.8921e-01, -3.2698e-01,  1.5247e+00, -2.1728e+00,  3.2363e-01,\n",
      "         -3.8881e-01,  3.4906e-02,  4.9706e-01, -1.8624e-01, -4.7725e-01,\n",
      "          1.5139e+00,  6.7868e-01, -1.2541e-02,  2.4135e+00, -2.0801e-01,\n",
      "          5.4333e-01, -3.6034e-01, -3.1781e-01, -6.1943e-01, -2.5598e-01,\n",
      "          3.0738e-01, -5.2291e-01,  6.7347e-01, -9.2570e-02, -4.5801e-01,\n",
      "         -1.0803e+00, -1.7933e-01, -2.7392e-01,  2.0282e+00, -3.7904e-01,\n",
      "         -2.2497e-01, -1.0957e-01, -8.4372e-01, -5.7906e-01, -3.6335e-01,\n",
      "         -3.6531e-01, -2.8004e-02, -2.9852e-01, -6.2006e-01,  9.2123e-01,\n",
      "         -2.0066e-01,  7.3081e-01,  1.0841e+00,  2.1865e+00, -1.3172e+00,\n",
      "         -1.8190e-01, -3.3528e-01,  2.7822e-01,  1.5995e+00,  8.9308e-01,\n",
      "          1.3993e+00,  1.3636e-01,  1.7798e+00, -6.5286e-01, -8.0986e-01,\n",
      "         -2.9357e-01, -1.3385e-01,  3.3044e-01,  5.5400e-01, -1.1420e-01,\n",
      "         -6.0385e-01, -1.8125e-01, -6.6330e-02,  4.2026e-01, -5.1759e-01,\n",
      "         -2.4628e-01, -7.9520e-02, -3.0819e-01, -3.3215e-01, -3.2461e-01,\n",
      "         -6.6146e-02,  7.1694e-01, -3.8603e-01,  9.0267e-01, -1.0062e-01,\n",
      "         -3.7660e-01,  5.8087e-01, -1.0516e+00, -4.2481e-01, -4.4222e-01,\n",
      "         -1.8251e-01, -5.2091e-01, -1.4306e-01, -3.1551e-01,  1.8888e-01,\n",
      "          1.9679e-02, -2.8876e-01, -1.8010e-01, -3.0276e-01,  7.3489e-01,\n",
      "         -3.1253e-01, -8.7559e-01,  1.8235e+00,  1.0913e+00, -5.8274e-01,\n",
      "          5.8023e-01,  8.6666e-01, -3.6777e-01, -4.2763e-01, -3.0018e-01,\n",
      "         -3.4904e-01]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "latent = [storage.latent[0].detach().clone()]\n",
    "latent[0].requires_grad = True\n",
    "\n",
    "h = storage.hidden_states[0].detach()\n",
    "for i in range(storage.actions.shape[0]):\n",
    "    # reset hidden state of the GRU when we reset the task\n",
    "    if storage.done[i] == 1:\n",
    "        print(i, 'Resetting', storage.done[i+1], storage.latent[i+1])\n",
    "        h = agent.actor_critic.encoder.reset_hidden(h, storage.done[i])\n",
    "\n",
    "        # also add the reset latent for the start of the next episode\n",
    "        _latent = storage.latent[i].detach().clone()\n",
    "        _latent.requires_grad = True\n",
    "        latent.append(_latent)\n",
    "    # not sure why this is i + 1?\n",
    "    # h = self.actor_critic.encoder.reset_hidden(h, policy_storage.done[i + 1])\n",
    "\n",
    "    _, tm, tl, h = agent.actor_critic.encoder(\n",
    "        storage.actions.float()[i:i + 1],\n",
    "        storage.next_state[i:i + 1],\n",
    "        storage.rewards_raw[i:i + 1],\n",
    "        h,\n",
    "        sample=False,\n",
    "        return_prior=False,\n",
    "        detach_every=None\n",
    "    )\n",
    "\n",
    "    latent.append(torch.cat((tm, tl), dim = -1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(256, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storage.latent[501] - \n",
    "(torch.cat(latent) == storage.latent[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1226.3718, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cat(storage.latent) - torch.cat(latent)).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 500, 1000, 1500, 2000]), array([0, 0, 0, 0]), array([0, 0, 0, 0]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(storage.done.cpu().detach().numpy()==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  501, 1002, 1503])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.where(torch.cat(storage.latent).cpu().detach().numpy()==storage.latent[0].cpu().detach().numpy()[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0036862 ,  0.05677195,  0.03968116, -0.00554151, -0.08180048,\n",
       "       -0.01013042,  0.07082925,  0.00790204,  0.0423823 ,  0.11912974,\n",
       "       -0.00084364, -0.02503387, -0.01213983, -0.06234327, -0.09946632,\n",
       "       -0.01211663, -0.01150087, -0.0718758 ,  0.04626398, -0.12751377,\n",
       "        0.00579331,  0.06083824,  0.09590048, -0.06088419,  0.04160011,\n",
       "        0.07273776, -0.05615995,  0.09611335, -0.03827709,  0.03161815,\n",
       "       -0.02291975,  0.02557641, -0.05081708,  0.03965855,  0.0675004 ,\n",
       "       -0.05241253,  0.00993486, -0.01231433,  0.00968155,  0.00498494,\n",
       "        0.00644882, -0.06429251,  0.05148085, -0.01024368,  0.02796842,\n",
       "        0.06069341,  0.00942255, -0.08616532,  0.01513379, -0.05556767,\n",
       "        0.03445356,  0.06938666, -0.08415639, -0.06715772, -0.0333991 ,\n",
       "       -0.03908315,  0.09423542,  0.04387035,  0.06786837,  0.00878241,\n",
       "       -0.04184344,  0.0700689 , -0.0438891 ,  0.03982812, -0.05555326,\n",
       "       -0.02070116,  0.04923742,  0.08097896,  0.05160142, -0.04984171,\n",
       "        0.01384338, -0.02046478,  0.00464789,  0.06442557, -0.02648797,\n",
       "        0.01702189,  0.00236906,  0.06566771,  0.03303943,  0.05057612,\n",
       "        0.0059194 ,  0.00690701, -0.0339856 ,  0.06294107, -0.01390278,\n",
       "        0.03074104,  0.06917708, -0.04189073, -0.0928992 ,  0.00056272,\n",
       "       -0.07221302,  0.03744731,  0.03745345,  0.08296842,  0.02140561,\n",
       "        0.03022061,  0.0922054 ,  0.00712266,  0.07515287,  0.08064695,\n",
       "        0.08962908, -0.05981203,  0.01436408, -0.04609227, -0.08766651,\n",
       "       -0.06389316, -0.03958899, -0.07636113, -0.02274401, -0.0291741 ,\n",
       "        0.0437245 , -0.05478473,  0.02221571, -0.00372553,  0.06670766,\n",
       "        0.05473853, -0.0451456 , -0.09083718, -0.0284883 ,  0.00397786,\n",
       "        0.01556921, -0.02481675, -0.0859251 ,  0.06298453, -0.08576652,\n",
       "       -0.06981671,  0.0793637 ,  0.02734014,  0.03046584, -0.0226834 ,\n",
       "       -0.07151913,  0.0116038 , -0.0490665 , -0.01333231, -0.08310853,\n",
       "       -0.04487234,  0.06247525, -0.07676023, -0.08458295,  0.11286186,\n",
       "       -0.06025029,  0.02285198,  0.08773272,  0.03352804,  0.09526026,\n",
       "       -0.02278058, -0.04525433, -0.06319063, -0.01636359,  0.00994436,\n",
       "        0.04274976,  0.06140835, -0.05305464, -0.03319002,  0.04441289,\n",
       "       -0.03339377, -0.06434068, -0.08092574, -0.12768881,  0.02434884,\n",
       "       -0.098381  , -0.10393441, -0.02092854, -0.04625849, -0.02270662,\n",
       "        0.09709293, -0.08066262, -0.02380908,  0.05556102,  0.06675462,\n",
       "       -0.05407033, -0.02108658,  0.05038207, -0.04553741, -0.03647897,\n",
       "       -0.10338921,  0.016002  ,  0.05693621, -0.07361154,  0.09973609,\n",
       "        0.08914672,  0.06864211, -0.09186637, -0.03375587, -0.10235057,\n",
       "        0.01594407, -0.11935415,  0.08723918,  0.01691651, -0.01258091,\n",
       "        0.03349059, -0.04910622, -0.0349144 ,  0.06673464, -0.08265463,\n",
       "       -0.03346777,  0.02911447,  0.00065186, -0.07623612,  0.06369342,\n",
       "       -0.02755033, -0.02815671, -0.02017665, -0.0287859 , -0.00681957,\n",
       "       -0.04147539, -0.02694789, -0.0393717 ,  0.0100554 ,  0.06086742,\n",
       "       -0.01686463, -0.03740262,  0.02879287,  0.06744125,  0.0763437 ,\n",
       "       -0.12094289, -0.05352916, -0.03437365, -0.0449815 ,  0.00961548,\n",
       "        0.06182658, -0.10262242,  0.04099209, -0.09320863, -0.06478766,\n",
       "        0.03487696,  0.04796049, -0.06385911, -0.08062591, -0.00425444,\n",
       "        0.08362887,  0.06157197, -0.02887066,  0.03594273, -0.02599807,\n",
       "        0.00190274, -0.09153655, -0.0279313 ,  0.0507411 , -0.09478052,\n",
       "       -0.03491362, -0.07856815,  0.06414209,  0.02687328,  0.04990325,\n",
       "       -0.02624585,  0.00776307,  0.04084626, -0.01832153,  0.08017422,\n",
       "        0.01356571,  0.08431891, -0.01541552, -0.09796293, -0.04452179,\n",
       "       -0.03326124], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(storage.latent).cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0],\n",
      "        [   0,    0,    1],\n",
      "        [   0,    0,    2],\n",
      "        ...,\n",
      "        [1500,    0,  125],\n",
      "        [1500,    0,  126],\n",
      "        [1500,    0,  127]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = storage.hidden_states ==0\n",
    "indices = a.nonzero()\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.hidden_states.size()[-1]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "# while the whole continual env is not done\n",
    "# train on each env sequentially\n",
    "# periodically evaluate on all envs\n",
    "\n",
    "agent = ActorCritic(policy_net, encoder_net)\n",
    "cont_env = ContinualEnv(training_envs, 10)\n",
    "eval_freq = 10\n",
    "num_steps = 0\n",
    "eval_results = dict()\n",
    "while cont_env.cur_step < cont_env.steps_limit:\n",
    "    # do each env\n",
    "    obs = cont_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent is not None:\n",
    "            obs = torch.from_numpy(np.append(obs, 0).reshape(1, -1))[None,:,:].float().to(device)\n",
    "            if hidden_state is not None:\n",
    "                reward = torch.from_numpy(np.array(reward).reshape(1, -1))[None,:,:].float().to(device)\n",
    "                action = torch.from_numpy(action)[None, None,:].float().to(device)\n",
    "            _, act, hidden_state = agent.act(action, obs, reward, hidden_state)\n",
    "            action = act.cpu().detach().numpy()[0]\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        next_obs, reward, done, info = cont_env.step(action)\n",
    "        obs = next_obs\n",
    "\n",
    "        # periodically evaluate\n",
    "        if  (cont_env.cur_step + 1) % eval_freq == 0:\n",
    "            print(cont_env.cur_step, 'EVALUATING')\n",
    "            all_envs = cont_env._get_envs()\n",
    "            eval_results[cont_env.cur_step] = evaluate_all_envs(all_envs, num_episodes = 3)\n",
    "            eval_results[cont_env.cur_step]['task'] = cont_env.cur_seq_idx\n",
    "        \n",
    "    print(cont_env.cur_step, cont_env.cur_seq_idx)\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varibad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
